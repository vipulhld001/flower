{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZdR1WXDEGxY",
    "outputId": "34c4f998-2ca0-4252-e4f7-424c286a9224"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q flwr[simulation] torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "obhFyleoEM9F",
    "outputId": "99c58edc-bb25-469f-b692-769941f695ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda using PyTorch 1.13.1 and Flower 1.4.0\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "import time\n",
    "import flwr as fl\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")  # Try \"cuda\" to train on GPU\n",
    "print(\n",
    "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen RAM Free: 125.8 GB  | Proc size: 303.7 MB\n",
      "GPU RAM Free: 15926MB | Used: 242MB | Util   1% | Total 16376MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isn’t guaranteed\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    "   process = psutil.Process(os.getpid())\n",
    "   print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "   print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "printm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2L0M1hDEQ54",
    "outputId": "d7c63f7f-13e7-4f97-ff2d-97443878fa29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_CLIENTS = 10\n",
    "\n",
    "\n",
    "def load_datasets(num_clients: int):\n",
    "    # Download and transform CIFAR-10 (train and test)\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "    trainset = CIFAR10(\"./dataset\", train=True, download=True, transform=transform)\n",
    "    testset = CIFAR10(\"./dataset\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # Split training set into `num_clients` partitions to simulate different local datasets\n",
    "    partition_size = len(trainset) // num_clients\n",
    "    lengths = [partition_size] * num_clients\n",
    "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
    "\n",
    "    # Split each partition into train/val and create DataLoader\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for ds in datasets:\n",
    "        len_val = len(ds) // 10  # 10 % validation set\n",
    "        len_train = len(ds) - len_val\n",
    "        lengths = [len_train, len_val]\n",
    "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(42))\n",
    "        trainloaders.append(DataLoader(ds_train, batch_size=32, shuffle=True))\n",
    "        valloaders.append(DataLoader(ds_val, batch_size=32))\n",
    "    testloader = DataLoader(testset, batch_size=32)\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets(NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4q9gZPKEEVZ0"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def train(net, trainloader, epochs: int):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):  # Use the passed 'epochs' variable here\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss.item()  # Make sure to call .item() to get the scalar value\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch}: train loss {epoch_loss:.6f}, accuracy {epoch_acc:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DpDR8IdSEX0E"
   },
   "outputs": [],
   "source": [
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, valloader):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self):\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        epochs = config.get(\"epochs\", 1)\n",
    "        start_time = time.time()  # Start time measurement\n",
    "        train(self.net, self.trainloader, epochs)\n",
    "        training_time = time.time() - start_time  # Calculate duration\n",
    "        print(f\"Training time for Client {self.cid}: {training_time:.2f} seconds\")\n",
    "        return get_parameters(self.net), len(self.trainloader), {\"training_time\": training_time}\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "def client_fn(cid) -> FlowerClient:\n",
    "    net = Net().to(DEVICE)\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "    return FlowerClient(cid, net, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mSYL7mrZE21l"
   },
   "outputs": [],
   "source": [
    "from typing import Callable, Union\n",
    "\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    MetricsAggregationFn,\n",
    "    NDArrays,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    ")\n",
    "from flwr.server.client_manager import ClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.server.strategy.aggregate import aggregate, weighted_loss_avg\n",
    "\n",
    "\n",
    "class FedCustom(fl.server.strategy.Strategy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fraction_fit: float = 1.0,\n",
    "        fraction_evaluate: float = 1.0,\n",
    "        min_fit_clients: int = 2,\n",
    "        min_evaluate_clients: int = 2,\n",
    "        min_available_clients: int = 2,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.fraction_fit = fraction_fit\n",
    "        self.fraction_evaluate = fraction_evaluate\n",
    "        self.min_fit_clients = min_fit_clients\n",
    "        self.min_evaluate_clients = min_evaluate_clients\n",
    "        self.min_available_clients = min_available_clients\n",
    "        self.client_training_times = {}\n",
    "    def __repr__(self) -> str:\n",
    "        return \"FedCustom\"\n",
    "\n",
    "    def initialize_parameters(\n",
    "        self, client_manager: ClientManager\n",
    "    ) -> Optional[Parameters]:\n",
    "        \"\"\"Initialize global model parameters.\"\"\"\n",
    "        net = Net()\n",
    "        ndarrays = get_parameters(net)\n",
    "        return fl.common.ndarrays_to_parameters(ndarrays)\n",
    "\n",
    "    def configure_fit(self, server_round: int, parameters: Parameters, client_manager: ClientManager):\n",
    "        sample_size, min_num_clients = self.num_fit_clients(client_manager.num_available())\n",
    "        clients = client_manager.sample(num_clients=sample_size, min_num_clients=min_num_clients)\n",
    "        epochs_sc = 5\n",
    "        epochs_hl = 3\n",
    "\n",
    "        standard_config = {\"lr\": 0.001, \"epochs\": epochs_sc}\n",
    "        higher_lr_config = {\"lr\": 0.0001, \"epochs\": epochs_hl}\n",
    "        fit_configurations = []\n",
    "\n",
    "        for client in clients:\n",
    "            # Choose config based on the previous training time\n",
    "            last_time = self.client_training_times.get(client.cid, 0)  # Default to 0 if no time recorded\n",
    "            print(f\"This is the last time {last_time}\")\n",
    "            \n",
    "\n",
    "\n",
    "            config_to_use = standard_config if last_time < 13.8 else higher_lr_config\n",
    "            fit_configurations.append((client, FitIns(parameters, config_to_use)))\n",
    "\n",
    "        return fit_configurations\n",
    "\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, FitRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate fit results using weighted average.\"\"\"\n",
    "        for client, fit_res in results:\n",
    "            # Update training times for each client\n",
    "            self.client_training_times[client.cid] = fit_res.metrics.get(\"training_time\", 0)\n",
    "        weights_results = [\n",
    "            (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)\n",
    "            for _, fit_res in results\n",
    "        ]\n",
    "        parameters_aggregated = ndarrays_to_parameters(aggregate(weights_results))\n",
    "        metrics_aggregated = {}\n",
    "        return parameters_aggregated, metrics_aggregated\n",
    "\n",
    "\n",
    "    def configure_evaluate(\n",
    "        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n",
    "    ) -> List[Tuple[ClientProxy, EvaluateIns]]:\n",
    "        \"\"\"Configure the next round of evaluation.\"\"\"\n",
    "        if self.fraction_evaluate == 0.0:\n",
    "            return []\n",
    "        config = {}\n",
    "        evaluate_ins = EvaluateIns(parameters, config)\n",
    "\n",
    "        # Sample clients\n",
    "        sample_size, min_num_clients = self.num_evaluation_clients(\n",
    "            client_manager.num_available()\n",
    "        )\n",
    "        clients = client_manager.sample(\n",
    "            num_clients=sample_size, min_num_clients=min_num_clients\n",
    "        )\n",
    "\n",
    "        # Return client/config pairs\n",
    "        return [(client, evaluate_ins) for client in clients]\n",
    "\n",
    "    def aggregate_evaluate(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[ClientProxy, EvaluateRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]],\n",
    "    ) -> Tuple[Optional[float], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate evaluation losses using weighted average.\"\"\"\n",
    "\n",
    "        if not results:\n",
    "            return None, {}\n",
    "\n",
    "        loss_aggregated = weighted_loss_avg(\n",
    "            [\n",
    "                (evaluate_res.num_examples, evaluate_res.loss)\n",
    "                for _, evaluate_res in results\n",
    "            ]\n",
    "        )\n",
    "        metrics_aggregated = {}\n",
    "        return loss_aggregated, metrics_aggregated\n",
    "\n",
    "    def evaluate(\n",
    "        self, server_round: int, parameters: Parameters\n",
    "    ) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "        \"\"\"Evaluate global model parameters using an evalua\n",
    "        tion function.\"\"\"\n",
    "\n",
    "        # Let's assume we won't perform the global model evaluation on the server side.\n",
    "        return None\n",
    "\n",
    "    def num_fit_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Return sample size and required number of clients.\"\"\"\n",
    "        num_clients = int(num_available_clients * self.fraction_fit)\n",
    "        return max(num_clients, self.min_fit_clients), self.min_available_clients\n",
    "\n",
    "    def num_evaluation_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Use a fraction of available clients for evaluation.\"\"\"\n",
    "        num_clients = int(num_available_clients * self.fraction_evaluate)\n",
    "        return max(num_clients, self.min_evaluate_clients), self.min_available_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEVICE.type == \"cuda\":\n",
    "    # Use a single client to train the global model\n",
    "    client_resources = {\"num_gpus\": .25, \"num_cpus\": 2} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvJM3bH4HDth",
    "outputId": "a273cb4a-ef0a-40e3-f789-e8f1f37366ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-06-26 19:48:33,238 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
      "2024-06-26 19:48:37,510\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "INFO flwr 2024-06-26 19:48:39,429 | app.py:180 | Flower VCE: Ray initialized with resources: {'node:127.0.0.1': 1.0, 'memory': 77768510055.0, 'object_store_memory': 37615075737.0, 'accelerator_type:RTX': 1.0, 'GPU': 1.0, 'CPU': 32.0, 'node:__internal_head__': 1.0}\n",
      "INFO flwr 2024-06-26 19:48:39,431 | server.py:86 | Initializing global parameters\n",
      "INFO flwr 2024-06-26 19:48:39,437 | server.py:269 | Using initial parameters provided by strategy\n",
      "INFO flwr 2024-06-26 19:48:39,438 | server.py:88 | Evaluating initial parameters\n",
      "INFO flwr 2024-06-26 19:48:39,439 | server.py:101 | FL starting\n",
      "DEBUG flwr 2024-06-26 19:48:39,441 | server.py:218 | fit_round 1: strategy sampled 10 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the last time 0\n",
      "This is the last time 0\n",
      "This is the last time 0\n",
      "This is the last time 0\n",
      "This is the last time 0\n",
      "This is the last time 0\n",
      "This is the last time 0\n",
      "This is the last time 0\n",
      "This is the last time 0\n",
      "This is the last time 0\n",
      "\u001b[36m(launch_and_fit pid=22360)\u001b[0m [Client 0] fit, config: {'lr': 0.001, 'epochs': 5}\n",
      "\u001b[36m(launch_and_fit pid=8476)\u001b[0m Epoch 0: train loss 0.065231, accuracy 0.226667\n",
      "\u001b[36m(launch_and_fit pid=33720)\u001b[0m [Client 5] fit, config: {'lr': 0.001, 'epochs': 5}\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=8476)\u001b[0m Epoch 2: train loss 0.051311, accuracy 0.399778\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=8476)\u001b[0m Training time for Client 4: 21.68 seconds\n",
      "\u001b[36m(launch_and_fit pid=33720)\u001b[0m Epoch 4: train loss 0.047180, accuracy 0.444000\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=21372)\u001b[0m [Client 8] fit, config: {'lr': 0.001, 'epochs': 5}\n",
      "\u001b[36m(launch_and_fit pid=32960)\u001b[0m Training time for Client 9: 23.49 seconds\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32960)\u001b[0m Epoch 4: train loss 0.047721, accuracy 0.439333\n",
      "\u001b[36m(launch_and_fit pid=21372)\u001b[0m Epoch 0: train loss 0.064799, accuracy 0.236000\n",
      "\u001b[36m(launch_and_fit pid=22728)\u001b[0m [Client 7] fit, config: {'lr': 0.001, 'epochs': 5}\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=21372)\u001b[0m Epoch 2: train loss 0.051318, accuracy 0.402222\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=21372)\u001b[0m Training time for Client 8: 22.90 seconds\n",
      "\u001b[36m(launch_and_fit pid=27404)\u001b[0m Epoch 4: train loss 0.046264, accuracy 0.455778\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=27452)\u001b[0m [Client 6] fit, config: {'lr': 0.001, 'epochs': 5}\n",
      "\u001b[36m(launch_and_fit pid=22728)\u001b[0m Training time for Client 7: 22.29 seconds\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=31444)\u001b[0m Epoch 0: train loss 0.066355, accuracy 0.210222\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=31444)\u001b[0m [Client 3] fit, config: {'lr': 0.001, 'epochs': 5}\n",
      "\u001b[36m(launch_and_fit pid=27452)\u001b[0m Epoch 2: train loss 0.052772, accuracy 0.373333\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=31444)\u001b[0m Training time for Client 3: 20.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-06-26 19:50:09,686 | server.py:232 | fit_round 1 received 10 results and 0 failures\n",
      "DEBUG flwr 2024-06-26 19:50:09,719 | server.py:168 | evaluate_round 1: strategy sampled 10 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(launch_and_fit pid=27452)\u001b[0m Epoch 4: train loss 0.047442, accuracy 0.442889\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_evaluate pid=17612)\u001b[0m [Client 8] evaluate, config: {}\n",
      "\u001b[36m(launch_and_fit pid=27452)\u001b[0m Training time for Client 6: 20.49 seconds\n",
      "\u001b[36m(launch_and_evaluate pid=31724)\u001b[0m [Client 1] evaluate, config: {}\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_evaluate pid=30540)\u001b[0m [Client 6] evaluate, config: {}\n",
      "\u001b[36m(launch_and_evaluate pid=3460)\u001b[0m [Client 4] evaluate, config: {}\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m [Client 9] evaluate, config: {}\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m [Client 3] evaluate, config: {}\n",
      "\u001b[33m(raylet)\u001b[0m Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1883, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1984, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 27, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 63, in test\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 15, in forward\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2281, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2177, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1832, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1833, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2071, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1089, in ray._raylet.store_task_errors\n",
      "  File \"python\\ray\\_raylet.pyx\", line 4575, in ray._raylet.CoreWorker.store_task_outputs\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 494, in serialize\n",
      "    return self._serialize_to_msgpack(value)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 468, in _serialize_to_msgpack\n",
      "    msgpack_data = MessagePackSerializer.dumps(value, _python_serializer)\n",
      "  File \"python\\ray\\includes/serialization.pxi\", line 175, in ray._raylet.MessagePackSerializer.dumps\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\msgpack\\__init__.py\", line 36, in packb\n",
      "    return Packer(**kwargs).pack(o)\n",
      "  File \"msgpack\\\\_packer.pyx\", line 120, in msgpack._cmsgpack.Packer.__cinit__\n",
      "MemoryError: Unable to allocate internal buffer.\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[33m(raylet)\u001b[0m Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1883, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1984, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 27, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 63, in test\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 12, in forward\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "RuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2281, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2177, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1832, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1833, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2071, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1089, in ray._raylet.store_task_errors\n",
      "  File \"python\\ray\\_raylet.pyx\", line 4575, in ray._raylet.CoreWorker.store_task_outputs\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 494, in serialize\n",
      "    return self._serialize_to_msgpack(value)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 468, in _serialize_to_msgpack\n",
      "    msgpack_data = MessagePackSerializer.dumps(value, _python_serializer)\n",
      "  File \"python\\ray\\includes/serialization.pxi\", line 175, in ray._raylet.MessagePackSerializer.dumps\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\msgpack\\__init__.py\", line 36, in packb\n",
      "    return Packer(**kwargs).pack(o)\n",
      "  File \"msgpack\\\\_packer.pyx\", line 120, in msgpack._cmsgpack.Packer.__cinit__\n",
      "MemoryError: Unable to allocate internal buffer.\n",
      "An unexpected internal error occurred while the worker was executing a task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m *** SIGABRT received at time=1719411777 ***\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FFA28B1DD61  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF996415136  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FFA28B1D492  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF6354C2297  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FFA285CDD31  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FFA2AF4AD6C  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FFA2AF33CC6  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FFA2AF48CDF  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FFA2AED5BEA  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FFA2AED2EF1  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FFA284D2BDC  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF9AC146BA7  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF9964E8B08  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF9964ED947  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF9964F3354  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF9964E8852  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF9964E9E71  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF995F96DF3  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF995F96D39  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF995D9A4EE  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF995DED4A5  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF995DA06CE  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF995E241FD  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF995E25D6F  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF995E13ADD  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF995E218AC  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF995DEDF78  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF995FA1123  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF995F99ACA  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF995F99FD4  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF9964ED49C  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m     @   00007FF9964F1F64  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m Fatal Python error: Aborted\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m \n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m Stack (most recent call first):\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\worker.py\", line 879 in main_loop\n",
      "\u001b[36m(launch_and_evaluate pid=25824)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\workers\\default_worker.py\", line 282 in <module>\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m fatal   : Memory allocation failure\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m fatal   : Memory allocation failure\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF996415136  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF9AC146BA7  (unknown)  CxxThrowException\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF9964E8B08  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF9964ED947  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF9964F3354  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF9964E8852  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF9964E9E71  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF995F96DF3  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF995F96D39  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF995D9A4EE  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF995DED4A5  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF995DA06CE  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF995E241FD  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF995E25D6F  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF995E13ADD  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF995E218AC  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF995DEDF78  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF995FA1123  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF995F99ACA  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF995F99FD4  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF9964ED49C  (unknown)  PyInit__raylet\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FF9964F1F64  (unknown)  PyInit__raylet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: c5b8c284d98d05d24f025d3d3fde4d76d59e7d8a01000000 Worker ID: 8208bc2c54578c5e7347132c1b03e29925363c540f03af4258d6b7e1 Node ID: 545ae7ca41736b854d81c0bc1e3553d05d65ae68e3db45c76c5d2cfb Worker IP address: 127.0.0.1 Worker port: 64911 Worker PID: 31760 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 10054. An existing connection was forcibly closed by the remote host. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[36m(launch_and_evaluate pid=4304)\u001b[0m [Client 3] evaluate, config: {}\n",
      "\u001b[33m(raylet)\u001b[0m Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1883, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1984, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 27, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 63, in test\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 12, in forward\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "RuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2281, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2177, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1832, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1833, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2071, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1089, in ray._raylet.store_task_errors\n",
      "  File \"python\\ray\\_raylet.pyx\", line 4575, in ray._raylet.CoreWorker.store_task_outputs\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 494, in serialize\n",
      "    return self._serialize_to_msgpack(value)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 468, in _serialize_to_msgpack\n",
      "    msgpack_data = MessagePackSerializer.dumps(value, _python_serializer)\n",
      "  File \"python\\ray\\includes/serialization.pxi\", line 175, in ray._raylet.MessagePackSerializer.dumps\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\msgpack\\__init__.py\", line 36, in packb\n",
      "    return Packer(**kwargs).pack(o)\n",
      "  File \"msgpack\\\\_packer.pyx\", line 120, in msgpack._cmsgpack.Packer.__cinit__\n",
      "MemoryError: Unable to allocate internal buffer.\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: dc35c089033030f2c15dd8501484fd6bb4f2e35801000000 Worker ID: a953638937fd9e0dd6ea60b6160fedc6cebc1f8ec5b4e222400e989b Node ID: 545ae7ca41736b854d81c0bc1e3553d05d65ae68e3db45c76c5d2cfb Worker IP address: 127.0.0.1 Worker port: 64870 Worker PID: 25824 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 10054. An existing connection was forcibly closed by the remote host. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[36m(launch_and_evaluate pid=10376)\u001b[0m [Client 9] evaluate, config: {}\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 30fac0bac9fddbcb22e328fb2242616a658ae52d01000000 Worker ID: 7a33eb2ea36fd9949fe4a3df37b9eb26886bc8050fe2ec87bd3e8a08 Node ID: 545ae7ca41736b854d81c0bc1e3553d05d65ae68e3db45c76c5d2cfb Worker IP address: 127.0.0.1 Worker port: 64952 Worker PID: 4304 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 10054. An existing connection was forcibly closed by the remote host. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(launch_and_evaluate pid=4304)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 459 in _conv_forward\n",
      "\u001b[36m(launch_and_evaluate pid=4304)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 463 in forward\n",
      "\u001b[36m(launch_and_evaluate pid=4304)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194 in _call_impl\n",
      "\u001b[36m(launch_and_evaluate pid=4304)\u001b[0m   File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 13 in forward\n",
      "\u001b[36m(launch_and_evaluate pid=4304)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194 in _call_impl\n",
      "\u001b[36m(launch_and_evaluate pid=4304)\u001b[0m   File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 63 in test\n",
      "\u001b[36m(launch_and_evaluate pid=4304)\u001b[0m   File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 27 in evaluate\n",
      "\u001b[36m(launch_and_evaluate pid=4304)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321 in _evaluate\n",
      "\u001b[36m(launch_and_evaluate pid=4304)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205 in maybe_call_evaluate\n",
      "\u001b[36m(launch_and_evaluate pid=4304)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160 in launch_and_evaluate\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m *** SIGABRT received at time=1719411777 ***\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m     @   00007FFA284D2BDC  (unknown)  (unknown)\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m Fatal Python error: Aborted\n",
      "\u001b[36m(launch_and_evaluate pid=31760)\u001b[0m \n",
      "\u001b[36m(launch_and_evaluate pid=4304)\u001b[0m Stack (most recent call first):\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_evaluate pid=4304)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\worker.py\", line 879 in main_loop\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_evaluate pid=4304)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\workers\\default_worker.py\", line 282 in <module>\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_evaluate pid=4304)\u001b[0m fatal   : Memory allocation failure\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_evaluate pid=10376)\u001b[0m *** SIGABRT received at time=1719411788 ***\n",
      "\u001b[36m(launch_and_evaluate pid=10376)\u001b[0m Fatal Python error: Aborted\n",
      "\u001b[36m(launch_and_evaluate pid=10376)\u001b[0m \n",
      "\u001b[36m(launch_and_evaluate pid=10376)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\workers\\default_worker.py\"\n",
      "\u001b[36m(launch_and_evaluate pid=10376)\u001b[0m , line 282 in <module>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(launch_and_evaluate pid=30352)\u001b[0m [Client 9] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(launch_and_evaluate pid=10376)\u001b[0m     @   00007FF9964F1F64  (unknown)  (unknown)\u001b[32m [repeated 32x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_evaluate pid=10376)\u001b[0m Stack (most recent call first):\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_evaluate pid=10376)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\worker.py\", line 879 in main_loop\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_evaluate pid=10376)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\workers\\default_worker.py\", line 282 in <module>\n",
      "\u001b[36m(launch_and_evaluate pid=30352)\u001b[0m fatal   : Memory allocation failure\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_evaluate pid=30352)\u001b[0m *** SIGABRT received at time=1719411799 ***\n",
      "\u001b[36m(launch_and_evaluate pid=30352)\u001b[0m Fatal Python error: Aborted\n",
      "\u001b[36m(launch_and_evaluate pid=30352)\u001b[0m \n",
      "\u001b[36m(launch_and_evaluate pid=30352)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\workers\\default_worker.py\", line 282 in <module>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1883, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1984, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 27, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 61, in test\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 628, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 671, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 61, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 265, in default_collate\n",
      "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 143, in collate\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 143, in <listcomp>\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 120, in collate\n",
      "    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 163, in collate_tensor_fn\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "RuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 393216 bytes.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2281, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2177, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1832, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1833, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2071, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1089, in ray._raylet.store_task_errors\n",
      "  File \"python\\ray\\_raylet.pyx\", line 4575, in ray._raylet.CoreWorker.store_task_outputs\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 494, in serialize\n",
      "    return self._serialize_to_msgpack(value)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 468, in _serialize_to_msgpack\n",
      "    msgpack_data = MessagePackSerializer.dumps(value, _python_serializer)\n",
      "  File \"python\\ray\\includes/serialization.pxi\", line 175, in ray._raylet.MessagePackSerializer.dumps\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\msgpack\\__init__.py\", line 36, in packb\n",
      "    return Packer(**kwargs).pack(o)\n",
      "  File \"msgpack\\\\_packer.pyx\", line 120, in msgpack._cmsgpack.Packer.__cinit__\n",
      "MemoryError: Unable to allocate internal buffer.\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 8f580fdf41deca5f0e14262a54bf50da64372b0101000000 Worker ID: 3c6f565053b895337dc3a164db5ddf4410584d513e7acfbede51fe2b Node ID: 545ae7ca41736b854d81c0bc1e3553d05d65ae68e3db45c76c5d2cfb Worker IP address: 127.0.0.1 Worker port: 64960 Worker PID: 10376 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 10054. An existing connection was forcibly closed by the remote host. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[36m(launch_and_evaluate pid=4224)\u001b[0m [Client 3] evaluate, config: {}\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 6b04fb0d2a71c533bf181b9efdf94a7957af166d01000000 Worker ID: 2191ab6495bd09a50e99bab97f6d40f8212d195e27d26062c4d556f5 Node ID: 545ae7ca41736b854d81c0bc1e3553d05d65ae68e3db45c76c5d2cfb Worker IP address: 127.0.0.1 Worker port: 64977 Worker PID: 4224 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. Worker exits with an exit code None. Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1883, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1984, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 27, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 61, in test\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 628, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 671, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 61, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 265, in default_collate\n",
      "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 143, in collate\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 143, in <listcomp>\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 120, in collate\n",
      "    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 163, in collate_tensor_fn\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "RuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 393216 bytes.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2281, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2177, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1832, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1833, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2071, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1089, in ray._raylet.store_task_errors\n",
      "  File \"python\\ray\\_raylet.pyx\", line 4575, in ray._raylet.CoreWorker.store_task_outputs\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 494, in serialize\n",
      "    return self._serialize_to_msgpack(value)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 468, in _serialize_to_msgpack\n",
      "    msgpack_data = MessagePackSerializer.dumps(value, _python_serializer)\n",
      "  File \"python\\ray\\includes/serialization.pxi\", line 175, in ray._raylet.MessagePackSerializer.dumps\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\msgpack\\__init__.py\", line 36, in packb\n",
      "    return Packer(**kwargs).pack(o)\n",
      "  File \"msgpack\\\\_packer.pyx\", line 120, in msgpack._cmsgpack.Packer.__cinit__\n",
      "MemoryError: Unable to allocate internal buffer.\n",
      "An unexpected internal error occurred while the worker was executing a task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(launch_and_evaluate pid=30352)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\workers\\default_worker.py\", line 282 in <module>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: d7bedbccd5c5391364a6653c81777704b926f6a101000000 Worker ID: be1973de6974082cd9b90c8dd73b32e10d66908844a33f27b44cd51b Node ID: 545ae7ca41736b854d81c0bc1e3553d05d65ae68e3db45c76c5d2cfb Worker IP address: 127.0.0.1 Worker port: 64970 Worker PID: 30352 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 10054. An existing connection was forcibly closed by the remote host. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[36m(launch_and_evaluate pid=22380)\u001b[0m [Client 9] evaluate, config: {}\n",
      "\u001b[33m(raylet)\u001b[0m Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1883, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1984, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 27, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 61, in test\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 628, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 671, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 61, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 265, in default_collate\n",
      "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 143, in collate\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 143, in <listcomp>\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 120, in collate\n",
      "    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 163, in collate_tensor_fn\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "RuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 393216 bytes.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2281, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2177, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1832, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1833, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2071, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1089, in ray._raylet.store_task_errors\n",
      "  File \"python\\ray\\_raylet.pyx\", line 4575, in ray._raylet.CoreWorker.store_task_outputs\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 494, in serialize\n",
      "    return self._serialize_to_msgpack(value)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 468, in _serialize_to_msgpack\n",
      "    msgpack_data = MessagePackSerializer.dumps(value, _python_serializer)\n",
      "  File \"python\\ray\\includes/serialization.pxi\", line 175, in ray._raylet.MessagePackSerializer.dumps\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\msgpack\\__init__.py\", line 36, in packb\n",
      "    return Packer(**kwargs).pack(o)\n",
      "  File \"msgpack\\\\_packer.pyx\", line 120, in msgpack._cmsgpack.Packer.__cinit__\n",
      "MemoryError: Unable to allocate internal buffer.\n",
      "An unexpected internal error occurred while the worker was executing a task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(launch_and_evaluate pid=20192)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 459 in _conv_forward\n",
      "\u001b[36m(launch_and_evaluate pid=20192)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 463 in forward\n",
      "\u001b[36m(launch_and_evaluate pid=20192)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194 in _call_impl\n",
      "\u001b[36m(launch_and_evaluate pid=20192)\u001b[0m   File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 12 in forward\n",
      "\u001b[36m(launch_and_evaluate pid=20192)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194 in _call_impl\n",
      "\u001b[36m(launch_and_evaluate pid=20192)\u001b[0m   File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 63 in test\n",
      "\u001b[36m(launch_and_evaluate pid=20192)\u001b[0m   File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 27 in evaluate\n",
      "\u001b[36m(launch_and_evaluate pid=20192)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321 in _evaluate\n",
      "\u001b[36m(launch_and_evaluate pid=20192)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205 in maybe_call_evaluate\n",
      "\u001b[36m(launch_and_evaluate pid=20192)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160 in launch_and_evaluate\n",
      "\u001b[36m(launch_and_evaluate pid=30352)\u001b[0m     @   00007FF9964F1F64  (unknown)  (unknown)\u001b[32m [repeated 32x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_evaluate pid=20192)\u001b[0m Stack (most recent call first):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_evaluate pid=20192)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\worker.py\", line 879 in main_loop\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_evaluate pid=4224)\u001b[0m fatal   : Memory allocation failure\u001b[32m [repeated 35x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_evaluate pid=20192)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\workers\\default_worker.py\", line 282 in <module>\n",
      "\u001b[36m(launch_and_evaluate pid=22380)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\workers\\default_worker.py\", line 282 in <module>\n",
      "ERROR flwr 2024-06-26 19:53:30,893 | ray_client_proxy.py:104 | The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 719e9ddcb7e91b511e1767bff0d3af3d5c72d24801000000 Worker ID: 4556dccfd0eb2f3765643a4e83197e96592fee1cf82baf85db5f76f6 Node ID: 545ae7ca41736b854d81c0bc1e3553d05d65ae68e3db45c76c5d2cfb Worker IP address: 127.0.0.1 Worker port: 64988 Worker PID: 20192 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 10054. An existing connection was forcibly closed by the remote host. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[36m(launch_and_evaluate pid=20192)\u001b[0m [Client 3] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-06-26 19:53:31,102 | ray_client_proxy.py:104 | The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "DEBUG flwr 2024-06-26 19:53:31,107 | server.py:182 | evaluate_round 1 received 8 results and 2 failures\n",
      "DEBUG flwr 2024-06-26 19:53:31,110 | server.py:218 | fit_round 2: strategy sampled 10 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: bfacc84ab396c91e3623e76bb38414633d2244f801000000 Worker ID: 6d23067eb7587edfcce4c26c471636ee7c8fa1eb9490d310e44de4c2 Node ID: 545ae7ca41736b854d81c0bc1e3553d05d65ae68e3db45c76c5d2cfb Worker IP address: 127.0.0.1 Worker port: 64995 Worker PID: 22380 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 10054. An existing connection was forcibly closed by the remote host. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "This is the last time 23.490320444107056\n",
      "This is the last time 21.794506311416626\n",
      "This is the last time 21.924933195114136\n",
      "This is the last time 20.490160942077637\n",
      "This is the last time 22.2886061668396\n",
      "This is the last time 20.205161809921265\n",
      "This is the last time 22.895992517471313\n",
      "This is the last time 23.315999031066895\n",
      "This is the last time 21.68092632293701\n",
      "This is the last time 22.724000215530396\n",
      "\u001b[36m(launch_and_fit pid=22008)\u001b[0m [Client 5] fit, config: {'lr': 0.0001, 'epochs': 3}\n",
      "\u001b[33m(raylet)\u001b[0m Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1883, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1984, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 148, in launch_and_fit\n",
      "    return maybe_call_fit(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 184, in maybe_call_fit\n",
      "    return client.fit(fit_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 297, in _fit\n",
      "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 17, in fit\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 41, in train\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 12, in forward\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2281, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2177, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1832, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1833, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2071, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1089, in ray._raylet.store_task_errors\n",
      "  File \"python\\ray\\_raylet.pyx\", line 4575, in ray._raylet.CoreWorker.store_task_outputs\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 494, in serialize\n",
      "    return self._serialize_to_msgpack(value)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 468, in _serialize_to_msgpack\n",
      "    msgpack_data = MessagePackSerializer.dumps(value, _python_serializer)\n",
      "  File \"python\\ray\\includes/serialization.pxi\", line 175, in ray._raylet.MessagePackSerializer.dumps\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\msgpack\\__init__.py\", line 36, in packb\n",
      "    return Packer(**kwargs).pack(o)\n",
      "  File \"msgpack\\\\_packer.pyx\", line 120, in msgpack._cmsgpack.Packer.__cinit__\n",
      "MemoryError: Unable to allocate internal buffer.\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: e63fbd39fd234e3c13c64f866184e050e404e8d101000000 Worker ID: eb604781b3961404541e28ba851b26fe07204383a28ebddcd064c20f Node ID: 545ae7ca41736b854d81c0bc1e3553d05d65ae68e3db45c76c5d2cfb Worker IP address: 127.0.0.1 Worker port: 65018 Worker PID: 30224 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. Worker exits with an exit code None. Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1883, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1984, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 148, in launch_and_fit\n",
      "    return maybe_call_fit(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 184, in maybe_call_fit\n",
      "    return client.fit(fit_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 297, in _fit\n",
      "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 17, in fit\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 41, in train\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 12, in forward\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2281, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2177, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1832, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1833, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2071, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1089, in ray._raylet.store_task_errors\n",
      "  File \"python\\ray\\_raylet.pyx\", line 4575, in ray._raylet.CoreWorker.store_task_outputs\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 494, in serialize\n",
      "    return self._serialize_to_msgpack(value)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 468, in _serialize_to_msgpack\n",
      "    msgpack_data = MessagePackSerializer.dumps(value, _python_serializer)\n",
      "  File \"python\\ray\\includes/serialization.pxi\", line 175, in ray._raylet.MessagePackSerializer.dumps\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\msgpack\\__init__.py\", line 36, in packb\n",
      "    return Packer(**kwargs).pack(o)\n",
      "  File \"msgpack\\\\_packer.pyx\", line 120, in msgpack._cmsgpack.Packer.__cinit__\n",
      "MemoryError: Unable to allocate internal buffer.\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: f575053972dc36121c6b3f67637af5e63b38e15801000000 Worker ID: b43d79a3f4161ccfd60d1cd49d73fa6a454c67ab86b15c4b8e64426b Node ID: 545ae7ca41736b854d81c0bc1e3553d05d65ae68e3db45c76c5d2cfb Worker IP address: 127.0.0.1 Worker port: 65010 Worker PID: 22008 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 10054. An existing connection was forcibly closed by the remote host. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(launch_and_fit pid=22008)\u001b[0m   File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 41 in train\n",
      "\u001b[36m(launch_and_fit pid=22008)\u001b[0m   File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 17 in fit\n",
      "\u001b[36m(launch_and_fit pid=22008)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 297 in _fit\n",
      "\u001b[36m(launch_and_fit pid=22008)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 184 in maybe_call_fit\n",
      "\u001b[36m(launch_and_fit pid=22008)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 148 in launch_and_fit\n",
      "\u001b[36m(launch_and_fit pid=22008)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 459 in _conv_forward\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=22008)\u001b[0m   File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 12 in forward\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=22008)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194 in _call_impl\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_evaluate pid=22380)\u001b[0m   File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 63 in test\n",
      "\u001b[36m(launch_and_evaluate pid=22380)\u001b[0m   File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 27 in evaluate\n",
      "\u001b[36m(launch_and_evaluate pid=22380)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321 in _evaluate\n",
      "\u001b[36m(launch_and_evaluate pid=22380)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205 in maybe_call_evaluate\n",
      "\u001b[36m(launch_and_evaluate pid=22380)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160 in launch_and_evaluate\n",
      "\u001b[36m(launch_and_fit pid=22008)\u001b[0m Stack (most recent call first):\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=22008)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\worker.py\", line 879 in main_loop\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=22008)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\workers\\default_worker.py\", line 282 in <module>\n",
      "\u001b[36m(launch_and_fit pid=30224)\u001b[0m fatal   : Memory allocation failure\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modul\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m e\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m s\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m \\c\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m onv.py\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m \"\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m , line 463 in \n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m f\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m orwar\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m d\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m   File \n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m \"c:\\Users\\Adm\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m i\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m n\\\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m a\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m nac\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m o\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m n\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m d\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m a\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m 3\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m   File \"C:\\Users\\Admin\\AppData\\Loca\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m l\\Temp\\ipykernel_5312\\1380938485.py\", line \n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m 17 in f\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m it\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m   File \"c\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m :\\\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m Us\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m er\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m s\\Admi\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m n\\a\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m on\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m \\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194 in _call_impl\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\workers\\default_worker.py\", line 282 in <module>\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m 3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 297 in _fit\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m     obj = pickle.loads(in_band)\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m MemoryError\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m Unable to allocate internal buffer.\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 222, in _deserialize_msgpack_data\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m     msgpack_data, pickle5_data = split_buffer(data)\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m   File \"python\\ray\\includes/serialization.pxi\", line 206, in ray._raylet.split_buffer\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m   File \"msgpack\\\\_unpacker.pyx\", line 372, in msgpack._cmsgpack.Unpacker.__init__\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m MemoryError: Unable to allocate internal buffer.\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m Unable to allocate internal buffer.\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 222, in _deserialize_msgpack_data\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m     msgpack_data, pickle5_data = split_buffer(data)\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m   File \"python\\ray\\includes/serialization.pxi\", line 206, in ray._raylet.split_buffer\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m   File \"msgpack\\\\_unpacker.pyx\", line 372, in msgpack._cmsgpack.Unpacker.__init__\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m MemoryError: Unable to allocate internal buffer.\n",
      "\u001b[36m(launch_and_fit pid=2096)\u001b[0m Windows fatal exception: stack overflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1836, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1870, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 968, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: \n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "MemoryError\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2281, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2177, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1832, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1833, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2071, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1089, in ray._raylet.store_task_errors\n",
      "  File \"python\\ray\\_raylet.pyx\", line 4575, in ray._raylet.CoreWorker.store_task_outputs\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 494, in serialize\n",
      "    return self._serialize_to_msgpack(value)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 468, in _serialize_to_msgpack\n",
      "    msgpack_data = MessagePackSerializer.dumps(value, _python_serializer)\n",
      "  File \"python\\ray\\includes/serialization.pxi\", line 175, in ray._raylet.MessagePackSerializer.dumps\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\msgpack\\__init__.py\", line 36, in packb\n",
      "    return Packer(**kwargs).pack(o)\n",
      "  File \"msgpack\\\\_packer.pyx\", line 120, in msgpack._cmsgpack.Packer.__cinit__\n",
      "MemoryError: Unable to allocate internal buffer.\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m [Client 6] fit, config: {'lr': 0.0001, 'epochs': 3}\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m   File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 41 in train\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m   File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 17 in fit\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 297 in _fit\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 184 in maybe_call_fit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 148 in launch_and_fit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 459 in _conv_forward\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m   File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 12 in forward\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=3264)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194 in _call_impl\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m Stack (most recent call first):\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\worker.py\", line 879 in main_loop\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32116)\u001b[0m fatal   : Memory allocation failure\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     from torchvision import datasets, io, models, ops, transforms, utils\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     from .densenet import *\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     import torch.utils.checkpoint as cp\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 934, in get_code\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 1033, in get_data\n",
      "ERROR flwr 2024-06-26 19:53:51,312 | ray_client_proxy.py:87 | \u001b[36mray::launch_and_fit()\u001b[39m (pid=26644, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1836, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1870, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 968, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: \n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "MemoryError\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m *** SIGABRT received at time=1719411831 ***\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FFA28B1DD61  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF996415136  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FFA28B1D492  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF6354C2297  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FFA285CDD31  (unknown)  UnhandledExceptionFilter\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FFA2AF4AD6C  (unknown)  memset\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FFA2AF33CC6  (unknown)  _C_specific_handler\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FFA2AF48CDF  (unknown)  _chkstk\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FFA2AED5BEA  (unknown)  RtlRestoreContext\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FFA2AED2EF1  (unknown)  RtlRaiseException\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FFA284D2BDC  (unknown)  RaiseException\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF9AC146BA7  (unknown)  CxxThrowException\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF9964E8B08  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF9964ED947  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF9964F3354  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF9964E8852  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF9964E9E71  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF995F96DF3  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF995F96D39  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF995D9A4EE  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF995DED4A5  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF995DA06CE  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF995E241FD  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF995E25D6F  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF995E13ADD  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF995E218AC  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF995DEDF78  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF995FA1123  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF995F99ACA  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF995F99FD4  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF9964ED49C  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     @   00007FF9964F1F64  (unknown)  (unknown)\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m Fatal Python error: Aborted\n",
      "ERROR flwr 2024-06-26 19:53:51,443 | ray_client_proxy.py:87 | \u001b[36mray::launch_and_fit()\u001b[39m (pid=30184, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1836, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1870, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 968, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: \n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "MemoryError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1883, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1984, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 148, in launch_and_fit\n",
      "    return maybe_call_fit(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 184, in maybe_call_fit\n",
      "    return client.fit(fit_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 297, in _fit\n",
      "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 17, in fit\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 41, in train\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 12, in forward\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "RuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2281, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2177, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1832, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1833, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2071, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1089, in ray._raylet.store_task_errors\n",
      "  File \"python\\ray\\_raylet.pyx\", line 4575, in ray._raylet.CoreWorker.store_task_outputs\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 494, in serialize\n",
      "    return self._serialize_to_msgpack(value)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 468, in _serialize_to_msgpack\n",
      "    msgpack_data = MessagePackSerializer.dumps(value, _python_serializer)\n",
      "  File \"python\\ray\\includes/serialization.pxi\", line 175, in ray._raylet.MessagePackSerializer.dumps\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\msgpack\\__init__.py\", line 36, in packb\n",
      "    return Packer(**kwargs).pack(o)\n",
      "  File \"msgpack\\\\_packer.pyx\", line 120, in msgpack._cmsgpack.Packer.__cinit__\n",
      "MemoryError: Unable to allocate internal buffer.\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 1a275ff8b3e9efcd55f9b6ecfcda9fdb4f7fd7b101000000 Worker ID: 7a656b44fbe258a80d258d0189c6edee267345e965fa51aaa207cd2d Node ID: 545ae7ca41736b854d81c0bc1e3553d05d65ae68e3db45c76c5d2cfb Worker IP address: 127.0.0.1 Worker port: 65029 Worker PID: 3264 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 10054. An existing connection was forcibly closed by the remote host. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1836, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1870, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 968, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: \n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torchvision\\__init__.py\", line 5, in <module>\n",
      "    from torchvision import datasets, io, models, ops, transforms, utils\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torchvision\\models\\__init__.py\", line 3, in <module>\n",
      "    from .densenet import *\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torchvision\\models\\densenet.py\", line 9, in <module>\n",
      "    import torch.utils.checkpoint as cp\n",
      "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 934, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1033, in get_data\n",
      "MemoryError\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2281, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2177, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1832, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1833, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2071, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1089, in ray._raylet.store_task_errors\n",
      "  File \"python\\ray\\_raylet.pyx\", line 4575, in ray._raylet.CoreWorker.store_task_outputs\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 494, in serialize\n",
      "    return self._serialize_to_msgpack(value)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 468, in _serialize_to_msgpack\n",
      "    msgpack_data = MessagePackSerializer.dumps(value, _python_serializer)\n",
      "  File \"python\\ray\\includes/serialization.pxi\", line 175, in ray._raylet.MessagePackSerializer.dumps\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\msgpack\\__init__.py\", line 36, in packb\n",
      "    return Packer(**kwargs).pack(o)\n",
      "  File \"msgpack\\\\_packer.pyx\", line 120, in msgpack._cmsgpack.Packer.__cinit__\n",
      "MemoryError: Unable to allocate internal buffer.\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 64f7e7d4c1a04a9d313c62036dac46db304e819e01000000 Worker ID: a68267c93d5762323ee4cf72ea77c76e421f6a76bfce1c18fd8369fb Node ID: 545ae7ca41736b854d81c0bc1e3553d05d65ae68e3db45c76c5d2cfb Worker IP address: 127.0.0.1 Worker port: 65047 Worker PID: 32116 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. Worker exits with an exit code None. Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1883, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1984, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 148, in launch_and_fit\n",
      "    return maybe_call_fit(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 184, in maybe_call_fit\n",
      "    return client.fit(fit_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 297, in _fit\n",
      "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 17, in fit\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 41, in train\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 12, in forward\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "RuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2281, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2177, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1832, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1833, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2071, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1089, in ray._raylet.store_task_errors\n",
      "  File \"python\\ray\\_raylet.pyx\", line 4575, in ray._raylet.CoreWorker.store_task_outputs\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 494, in serialize\n",
      "    return self._serialize_to_msgpack(value)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 468, in _serialize_to_msgpack\n",
      "    msgpack_data = MessagePackSerializer.dumps(value, _python_serializer)\n",
      "  File \"python\\ray\\includes/serialization.pxi\", line 175, in ray._raylet.MessagePackSerializer.dumps\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\msgpack\\__init__.py\", line 36, in packb\n",
      "    return Packer(**kwargs).pack(o)\n",
      "  File \"msgpack\\\\_packer.pyx\", line 120, in msgpack._cmsgpack.Packer.__cinit__\n",
      "MemoryError: Unable to allocate internal buffer.\n",
      "An unexpected internal error occurred while the worker was executing a task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-06-26 19:53:53,229 | ray_client_proxy.py:87 | \u001b[36mray::launch_and_fit()\u001b[39m (pid=28252, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1836, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1870, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 968, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_cnn_train64_8.dll\" or one of its dependencies.\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 457, in subimport\n",
      "    __import__(name)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\__init__.py\", line 128, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_cnn_train64_8.dll\" or one of its dependencies.\n",
      "\u001b[36m(launch_and_fit pid=28252)\u001b[0m [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_cnn_train64_8.dll\" or one of its dependencies.\n",
      "\u001b[36m(launch_and_fit pid=28252)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 457, in subimport\n",
      "\u001b[36m(launch_and_fit pid=28252)\u001b[0m     __import__(name)\n",
      "\u001b[36m(launch_and_fit pid=28252)\u001b[0m     raise err\n",
      "\u001b[36m(launch_and_fit pid=28252)\u001b[0m OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_cnn_train64_8.dll\" or one of its dependencies.\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m d\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m a\n",
      "\u001b[36m(launch_and_fit pid=23656)\u001b[0m nac\n",
      "\u001b[36m(launch_and_fit pid=30184)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=28252)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\__init__.py\", line 128, in <module>\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=28252)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=28252)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=28252)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=28252)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=28252)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=28252)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=28252)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=28252)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=28252)\u001b[0m     obj = pickle.loads(in_band)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=30184)\u001b[0m MemoryError\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m Unable to allocate internal buffer.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m     msgpack_data, pickle5_data = split_buffer(data)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m   File \"python\\ray\\includes/serialization.pxi\", line 206, in ray._raylet.split_buffer\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m   File \"msgpack\\\\_unpacker.pyx\", line 372, in msgpack._cmsgpack.Unpacker.__init__\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m MemoryError: Unable to allocate internal buffer.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m Python path configuration:\n",
      "\u001b[33m(raylet)\u001b[0m   PYTHONHOME = (not set)\n",
      "\u001b[33m(raylet)\u001b[0m   PYTHONPATH = (not set)\n",
      "\u001b[33m(raylet)\u001b[0m   program name = 'c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\python.exe'\n",
      "\u001b[33m(raylet)\u001b[0m   isolated = 0\n",
      "\u001b[33m(raylet)\u001b[0m   environment = 1\n",
      "\u001b[33m(raylet)\u001b[0m   user site = 1\n",
      "\u001b[33m(raylet)\u001b[0m   import site = 1\n",
      "\u001b[33m(raylet)\u001b[0m   sys._base_executable = 'c:\\\\Users\\\\Admin\\\\anaconda3\\\\envs\\\\flwrpytorch\\\\python.exe'\n",
      "\u001b[33m(raylet)\u001b[0m   sys.base_prefix = 'c:\\\\Users\\\\Admin\\\\anaconda3\\\\envs\\\\flwrpytorch'\n",
      "\u001b[33m(raylet)\u001b[0m   sys.base_exec_prefix = 'c:\\\\Users\\\\Admin\\\\anaconda3\\\\envs\\\\flwrpytorch'\n",
      "\u001b[33m(raylet)\u001b[0m   sys.executable = 'c:\\\\Users\\\\Admin\\\\anaconda3\\\\envs\\\\flwrpytorch\\\\python.exe'\n",
      "\u001b[33m(raylet)\u001b[0m   sys.prefix = 'c:\\\\Users\\\\Admin\\\\anaconda3\\\\envs\\\\flwrpytorch'\n",
      "\u001b[33m(raylet)\u001b[0m   sys.exec_prefix = 'c:\\\\Users\\\\Admin\\\\anaconda3\\\\envs\\\\flwrpytorch'\n",
      "\u001b[33m(raylet)\u001b[0m   sys.path = [\n",
      "\u001b[33m(raylet)\u001b[0m   ]\n",
      "\u001b[33m(raylet)\u001b[0m Fatal Python error: init_fs_encoding: failed to get the Python codec of the filesystem encoding\n",
      "\u001b[33m(raylet)\u001b[0m Python runtime state: core initialized\n",
      "ERROR flwr 2024-06-26 19:53:53,329 | ray_client_proxy.py:87 | \u001b[36mray::launch_and_fit()\u001b[39m (pid=10088, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1836, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1870, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 968, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 457, in subimport\n",
      "    __import__(name)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\__init__.py\", line 128, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "\u001b[36m(launch_and_fit pid=10088)\u001b[0m [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "\u001b[36m(launch_and_fit pid=10088)\u001b[0m OSError: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "ERROR flwr 2024-06-26 19:53:54,945 | ray_client_proxy.py:87 | \u001b[36mray::launch_and_fit()\u001b[39m (pid=448, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1836, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1870, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 968, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 457, in subimport\n",
      "    __import__(name)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\__init__.py\", line 128, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(launch_and_fit pid=33668)\u001b[0m [Client 4] fit, config: {'lr': 0.0001, 'epochs': 3}\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 19:53:55,682\tERROR serialization.py:406 -- Unable to allocate internal buffer.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 292, in _deserialize_object\n",
      "    obj = self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 222, in _deserialize_msgpack_data\n",
      "    msgpack_data, pickle5_data = split_buffer(data)\n",
      "  File \"python\\ray\\includes/serialization.pxi\", line 206, in ray._raylet.split_buffer\n",
      "  File \"msgpack\\\\_unpacker.pyx\", line 372, in msgpack._cmsgpack.Unpacker.__init__\n",
      "MemoryError: Unable to allocate internal buffer.\n",
      "ERROR flwr 2024-06-26 19:53:55,692 | ray_client_proxy.py:87 | System error: Unable to allocate internal buffer.\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 292, in _deserialize_object\n",
      "    obj = self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 222, in _deserialize_msgpack_data\n",
      "    msgpack_data, pickle5_data = split_buffer(data)\n",
      "  File \"python\\ray\\includes/serialization.pxi\", line 206, in ray._raylet.split_buffer\n",
      "  File \"msgpack\\\\_unpacker.pyx\", line 372, in msgpack._cmsgpack.Unpacker.__init__\n",
      "MemoryError: Unable to allocate internal buffer.\n",
      "\n",
      "\u001b[33m(raylet)\u001b[0m     import ray\n",
      "\u001b[33m(raylet)\u001b[0m     from ray._private.worker import (  # noqa: E402,F401\n",
      "\u001b[33m(raylet)\u001b[0m     import ray.actor\n",
      "\u001b[33m(raylet)\u001b[0m     from ray.util.tracing.tracing_helper import (\n",
      "\u001b[33m(raylet)\u001b[0m     from ray.runtime_context import get_runtime_context\n",
      "\u001b[33m(raylet)\u001b[0m     from ray.runtime_env import RuntimeEnv\n",
      "\u001b[33m(raylet)\u001b[0m     from ray.runtime_env.runtime_env import RuntimeEnv, RuntimeEnvConfig  # noqa: E402,F401\n",
      "\u001b[33m(raylet)\u001b[0m     from ray._private.runtime_env.conda import get_uri as get_conda_uri\n",
      "\u001b[33m(raylet)\u001b[0m     from ray._private.runtime_env.packaging import Protocol, parse_uri\n",
      "\u001b[33m(raylet)\u001b[0m     class Protocol(Enum):\n",
      "\u001b[33m(raylet)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\enum.py\", line 265, in __new__\n",
      "\u001b[33m(raylet)\u001b[0m     setattr(enum_class, member_name, enum_member)\n",
      "\u001b[33m(raylet)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\enum.py\", line 428, in __setattr__\n",
      "\u001b[33m(raylet)\u001b[0m     super().__setattr__(name, value)\n",
      "\u001b[33m(raylet)\u001b[0m MemoryError: Out of memory interning an attribute name\n",
      "\u001b[33m(raylet)\u001b[0m     import argparse\n",
      "\u001b[33m(raylet)\u001b[0m     import shutil as _shutil\n",
      "\u001b[33m(raylet)\u001b[0m [2024-06-26 19:54:53,255 E 14752 22300] (raylet.exe) worker_pool.cc:550: Some workers of the worker process(27112) have not registered within the timeout. The process is dead, probably it crashed during start.\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m Stack (most recent call first):\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=21388)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\worker.py\", line 879 in main_loop\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=33668)\u001b[0m fatal   : Memory allocation failure\n",
      "\u001b[33m(raylet)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 934, in get_code\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 1033, in get_data\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=448)\u001b[0m [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "\u001b[36m(launch_and_fit pid=448)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 457, in subimport\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=448)\u001b[0m     __import__(name)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=448)\u001b[0m     raise err\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=448)\u001b[0m OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "\u001b[33m(raylet)\u001b[0m     'c:\\\\Users\\\\Admin\\\\anaconda3\\\\envs\\\\flwrpytorch',\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\argparse.py\", line 89, in <module>\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=448)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=448)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=448)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=448)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=448)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=448)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=448)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=448)\u001b[0m     obj = pickle.loads(in_band)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m MemoryError\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "ERROR flwr 2024-06-26 19:54:54,784 | ray_client_proxy.py:87 | \u001b[36mray::launch_and_fit()\u001b[39m (pid=23536, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1836, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1870, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 968, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_adv_infer64_8.dll\" or one of its dependencies.\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 457, in subimport\n",
      "    __import__(name)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\__init__.py\", line 128, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_adv_infer64_8.dll\" or one of its dependencies.\n",
      "\u001b[36m(launch_and_fit pid=23536)\u001b[0m [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_adv_infer64_8.dll\" or one of its dependencies.\n",
      "\u001b[36m(launch_and_fit pid=23536)\u001b[0m OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_adv_infer64_8.dll\" or one of its dependencies.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-06-26 19:54:56,051 E 23332 8380] logging.cc:104: Stack trace: \n",
      "\u001b[33m(raylet)\u001b[0m  unknown\n",
      "\u001b[33m(raylet)\u001b[0m *** SIGABRT received at time=1719411896 ***\n",
      "\u001b[33m(raylet)\u001b[0m     @   00007FFA28B1DD61  (unknown)  (unknown)\n",
      "\u001b[33m(raylet)\u001b[0m [2024-06-26 19:54:56,051 E 23332 8380] logging.cc:361: *** SIGABRT received at time=1719411896 ***\n",
      "\u001b[33m(raylet)\u001b[0m [2024-06-26 19:54:56,051 E 23332 8380] logging.cc:361:     @   00007FFA28B1DD61  (unknown)  (unknown)\n",
      "\u001b[33m(raylet)\u001b[0m Fatal Python error: Aborted\n",
      "\u001b[33m(raylet)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\worker.py\", line 2391 in connect\n",
      "\u001b[33m(raylet)\u001b[0m     import ray\n",
      "\u001b[33m(raylet)\u001b[0m     import ray._raylet  # noqa: E402\n",
      "\u001b[33m(raylet)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 169, in init ray._raylet\n",
      "\u001b[33m(raylet)\u001b[0m     from ray.core.generated.common_pb2 import (\n",
      "\u001b[33m(raylet)\u001b[0m     from google.protobuf import reflection as _reflection\n",
      "\u001b[33m(raylet)\u001b[0m     from google.protobuf import message_factory\n",
      "\u001b[33m(raylet)\u001b[0m     from google.protobuf.internal import python_message as message_impl\n",
      "\u001b[33m(raylet)\u001b[0m     from google.protobuf.internal import containers\n",
      "\u001b[33m(raylet)\u001b[0m     class RepeatedScalarFieldContainer(BaseContainer[_T], MutableSequence[_T]):\n",
      "\u001b[33m(raylet)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\", line 180, in RepeatedScalarFieldContainer\n",
      "\u001b[33m(raylet)\u001b[0m     other: Union['RepeatedScalarFieldContainer[_T]', Iterable[_T]],\n",
      "\u001b[33m(raylet)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\typing.py\", line 258, in inner\n",
      "\u001b[33m(raylet)\u001b[0m     return cached(*args, **kwds)\n",
      "\u001b[33m(raylet)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\typing.py\", line 358, in __getitem__\n",
      "\u001b[33m(raylet)\u001b[0m     parameters = tuple(_type_check(p, msg) for p in parameters)\n",
      "\u001b[33m(raylet)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\typing.py\", line 358, in <genexpr>\n",
      "\u001b[33m(raylet)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\typing.py\", line 139, in _type_check\n",
      "\u001b[33m(raylet)\u001b[0m     return ForwardRef(arg)\n",
      "\u001b[33m(raylet)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\typing.py\", line 500, in __init__\n",
      "\u001b[33m(raylet)\u001b[0m     code = compile(arg, '<string>', 'eval')\n",
      "ERROR flwr 2024-06-26 19:54:58,324 | ray_client_proxy.py:87 | \u001b[36mray::launch_and_fit()\u001b[39m (pid=24032, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1836, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1870, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 968, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 457, in subimport\n",
      "    __import__(name)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\__init__.py\", line 128, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "\u001b[36m(launch_and_fit pid=24032)\u001b[0m [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "\u001b[36m(launch_and_fit pid=24032)\u001b[0m OSError: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-06-26 19:54:55,705 E 14752 22300] (raylet.exe) worker_pool.cc:550: Some workers of the worker process(26604) have not registered within the timeout. The process is dead, probably it crashed during start.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m Stack (most recent call first):\n",
      "\u001b[36m(launch_and_fit pid=24032)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 457, in subimport\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=24032)\u001b[0m     __import__(name)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=24032)\u001b[0m     raise err\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=24032)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\__init__.py\", line 128, in <module>\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=24032)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=24032)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=24032)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=24032)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=24032)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=24032)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=24032)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=24032)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=24032)\u001b[0m     obj = pickle.loads(in_band)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m MemoryError\n",
      "\u001b[33m(raylet)\u001b[0m unknown\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m     @   00007FF9AC3D118F  (unknown)  (unknown)\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m [2024-06-26 19:54:56,051 E 23332 8380] logging.cc:361:     @   00007FF9AC3D118F  (unknown)  (unknown)\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m     parameters = tuple(_type_check(p, msg) for p in parameters)\n",
      "\u001b[33m(raylet)\u001b[0m [2024-06-26 19:55:54,992 E 14752 22300] (raylet.exe) worker_pool.cc:550: Some workers of the worker process(23332) have not registered within the timeout. The process is dead, probably it crashed during start.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-06-26 19:55:55,723 E 14752 22300] (raylet.exe) worker_pool.cc:550: Some workers of the worker process(18344) have not registered within the timeout. The process is dead, probably it crashed during start.\n",
      "ERROR flwr 2024-06-26 19:55:56,660 | ray_client_proxy.py:87 | \u001b[36mray::launch_and_fit()\u001b[39m (pid=25460, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1836, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1870, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 968, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 457, in subimport\n",
      "    __import__(name)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\__init__.py\", line 128, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "\u001b[36m(launch_and_fit pid=25460)\u001b[0m [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n",
      "\u001b[36m(launch_and_fit pid=25460)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(launch_and_fit pid=25460)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "\u001b[36m(launch_and_fit pid=25460)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[36m(launch_and_fit pid=25460)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "\u001b[36m(launch_and_fit pid=25460)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[36m(launch_and_fit pid=25460)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\n",
      "\u001b[36m(launch_and_fit pid=25460)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "\u001b[36m(launch_and_fit pid=25460)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\n",
      "\u001b[36m(launch_and_fit pid=25460)\u001b[0m     obj = pickle.loads(in_band)\n",
      "\u001b[36m(launch_and_fit pid=25460)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 457, in subimport\n",
      "\u001b[36m(launch_and_fit pid=25460)\u001b[0m     __import__(name)\n",
      "\u001b[36m(launch_and_fit pid=25460)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\__init__.py\", line 128, in <module>\n",
      "\u001b[36m(launch_and_fit pid=25460)\u001b[0m     raise err\n",
      "\u001b[36m(launch_and_fit pid=25460)\u001b[0m OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1836, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1870, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 968, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_cnn_train64_8.dll\" or one of its dependencies.\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 457, in subimport\n",
      "    __import__(name)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\__init__.py\", line 128, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_cnn_train64_8.dll\" or one of its dependencies.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2281, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2177, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1832, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1833, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2071, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1089, in ray._raylet.store_task_errors\n",
      "  File \"python\\ray\\_raylet.pyx\", line 4575, in ray._raylet.CoreWorker.store_task_outputs\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 494, in serialize\n",
      "    return self._serialize_to_msgpack(value)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 468, in _serialize_to_msgpack\n",
      "    msgpack_data = MessagePackSerializer.dumps(value, _python_serializer)\n",
      "  File \"python\\ray\\includes/serialization.pxi\", line 175, in ray._raylet.MessagePackSerializer.dumps\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\msgpack\\__init__.py\", line 36, in packb\n",
      "    return Packer(**kwargs).pack(o)\n",
      "  File \"msgpack\\\\_packer.pyx\", line 120, in msgpack._cmsgpack.Packer.__cinit__\n",
      "MemoryError: Unable to allocate internal buffer.\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 1ba98c96cfedf88ae68060603a6fc8365633e00101000000 Worker ID: 94aec81760f2792da7dfdde1cf2918d4483f16fdd084c4125a1c4500 Node ID: 545ae7ca41736b854d81c0bc1e3553d05d65ae68e3db45c76c5d2cfb Worker IP address: 127.0.0.1 Worker port: 65077 Worker PID: 21388 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 10054. An existing connection was forcibly closed by the remote host. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 34cb265c4b5d90b0868207cb783c4ec9752b88c601000000 Worker ID: 60c97109a17fabd73df5a907220e0308106a7eedd9fdf36b60d8d6df Node ID: 545ae7ca41736b854d81c0bc1e3553d05d65ae68e3db45c76c5d2cfb Worker IP address: 127.0.0.1 Worker port: 65306 Worker PID: 26892 Worker exit type: SYSTEM_ERROR Worker exit detail: The leased worker has unrecoverable failure. Worker is requested to be destroyed when it is returned. Worker exits with an exit code None. Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1836, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1870, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 968, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_cnn_train64_8.dll\" or one of its dependencies.\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 457, in subimport\n",
      "    __import__(name)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\__init__.py\", line 128, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_cnn_train64_8.dll\" or one of its dependencies.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2281, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2177, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1832, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1833, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2071, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1089, in ray._raylet.store_task_errors\n",
      "  File \"python\\ray\\_raylet.pyx\", line 4575, in ray._raylet.CoreWorker.store_task_outputs\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 494, in serialize\n",
      "    return self._serialize_to_msgpack(value)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 468, in _serialize_to_msgpack\n",
      "    msgpack_data = MessagePackSerializer.dumps(value, _python_serializer)\n",
      "  File \"python\\ray\\includes/serialization.pxi\", line 175, in ray._raylet.MessagePackSerializer.dumps\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\msgpack\\__init__.py\", line 36, in packb\n",
      "    return Packer(**kwargs).pack(o)\n",
      "  File \"msgpack\\\\_packer.pyx\", line 120, in msgpack._cmsgpack.Packer.__cinit__\n",
      "MemoryError: Unable to allocate internal buffer.\n",
      "An unexpected internal error occurred while the worker was executing a task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(launch_and_fit pid=26892)\u001b[0m Unable to allocate internal buffer.\n",
      "\u001b[36m(launch_and_fit pid=26892)\u001b[0m     msgpack_data, pickle5_data = split_buffer(data)\n",
      "\u001b[36m(launch_and_fit pid=26892)\u001b[0m   File \"python\\ray\\includes/serialization.pxi\", line 206, in ray._raylet.split_buffer\n",
      "\u001b[36m(launch_and_fit pid=26892)\u001b[0m   File \"msgpack\\\\_unpacker.pyx\", line 372, in msgpack._cmsgpack.Unpacker.__init__\n",
      "\u001b[36m(launch_and_fit pid=26892)\u001b[0m MemoryError: Unable to allocate internal buffer.\n",
      "\u001b[36m(launch_and_fit pid=26892)\u001b[0m Unable to allocate internal buffer.\n",
      "\u001b[36m(launch_and_fit pid=26892)\u001b[0m     msgpack_data, pickle5_data = split_buffer(data)\n",
      "\u001b[36m(launch_and_fit pid=26892)\u001b[0m   File \"python\\ray\\includes/serialization.pxi\", line 206, in ray._raylet.split_buffer\n",
      "\u001b[36m(launch_and_fit pid=26892)\u001b[0m   File \"msgpack\\\\_unpacker.pyx\", line 372, in msgpack._cmsgpack.Unpacker.__init__\n",
      "\u001b[36m(launch_and_fit pid=26892)\u001b[0m MemoryError: Unable to allocate internal buffer.\n",
      "\u001b[36m(launch_and_fit pid=12048)\u001b[0m *** SIGABRT received at time=1719411959 ***\n",
      "\u001b[36m(launch_and_fit pid=12048)\u001b[0m     @   00007FFA285CDD31  (unknown)  UnhandledExceptionFilter\n",
      "\u001b[36m(launch_and_fit pid=12048)\u001b[0m     @   00007FFA284D2BDC  (unknown)  RaiseException\n",
      "\u001b[36m(launch_and_fit pid=12048)\u001b[0m Fatal Python error: Aborted\n",
      "\u001b[36m(launch_and_fit pid=12048)\u001b[0m \n",
      "\u001b[36m(launch_and_fit pid=12048)\u001b[0m Stack (most recent call first):\n",
      "\u001b[36m(launch_and_fit pid=12048)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\worker.py\", line 879 in main_loop\n",
      "\u001b[36m(launch_and_fit pid=12048)\u001b[0m Stack (most recent call first):\n",
      "\u001b[36m(launch_and_fit pid=12048)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\worker.py\", line 879 in main_loop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 705c9fe83b2d3f75e1191b2516690cc3b7becfda01000000 Worker ID: e9981a923491408a58948138723269cb066f43f097498fad3994f987 Node ID: 545ae7ca41736b854d81c0bc1e3553d05d65ae68e3db45c76c5d2cfb Worker IP address: 127.0.0.1 Worker port: 65316 Worker PID: 12048 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 10054. An existing connection was forcibly closed by the remote host. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[33m(raylet)\u001b[0m Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1836, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1870, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 968, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_cnn_train64_8.dll\" or one of its dependencies.\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 457, in subimport\n",
      "    __import__(name)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\__init__.py\", line 128, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_cnn_train64_8.dll\" or one of its dependencies.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2281, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2177, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1832, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1833, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2071, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1089, in ray._raylet.store_task_errors\n",
      "  File \"python\\ray\\_raylet.pyx\", line 4575, in ray._raylet.CoreWorker.store_task_outputs\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 494, in serialize\n",
      "    return self._serialize_to_msgpack(value)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 468, in _serialize_to_msgpack\n",
      "    msgpack_data = MessagePackSerializer.dumps(value, _python_serializer)\n",
      "  File \"python\\ray\\includes/serialization.pxi\", line 175, in ray._raylet.MessagePackSerializer.dumps\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\msgpack\\__init__.py\", line 36, in packb\n",
      "    return Packer(**kwargs).pack(o)\n",
      "  File \"msgpack\\\\_packer.pyx\", line 120, in msgpack._cmsgpack.Packer.__cinit__\n",
      "MemoryError: Unable to allocate internal buffer.\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[33m(raylet)\u001b[0m Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1836, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1870, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 968, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_cnn_train64_8.dll\" or one of its dependencies.\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 457, in subimport\n",
      "    __import__(name)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\__init__.py\", line 128, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_cnn_train64_8.dll\" or one of its dependencies.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2281, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2177, in ray._raylet.execute_task_with_cancellation_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1832, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1833, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 2071, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1089, in ray._raylet.store_task_errors\n",
      "  File \"python\\ray\\_raylet.pyx\", line 4575, in ray._raylet.CoreWorker.store_task_outputs\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 494, in serialize\n",
      "    return self._serialize_to_msgpack(value)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 468, in _serialize_to_msgpack\n",
      "    msgpack_data = MessagePackSerializer.dumps(value, _python_serializer)\n",
      "  File \"python\\ray\\includes/serialization.pxi\", line 175, in ray._raylet.MessagePackSerializer.dumps\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\msgpack\\__init__.py\", line 36, in packb\n",
      "    return Packer(**kwargs).pack(o)\n",
      "  File \"msgpack\\\\_packer.pyx\", line 120, in msgpack._cmsgpack.Packer.__cinit__\n",
      "MemoryError: Unable to allocate internal buffer.\n",
      "An unexpected internal error occurred while the worker was executing a task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m     @   00007FF9964F1F64  (unknown)  (unknown)\u001b[32m [repeated 62x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_cnn_train64_8.dll\" or one of its dependencies.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 222, in _deserialize_msgpack_data\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m     obj = pickle.loads(in_band)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 457, in subimport\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m     __import__(name)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\workers\\default_worker.py\", line 282 in <module>\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m     raise err\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_cnn_train64_8.dll\" or one of its dependencies.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m Unable to allocate internal buffer.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m     msgpack_data, pickle5_data = split_buffer(data)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m   File \"python\\ray\\includes/serialization.pxi\", line 206, in ray._raylet.split_buffer\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m   File \"msgpack\\\\_unpacker.pyx\", line 372, in msgpack._cmsgpack.Unpacker.__init__\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m MemoryError: Unable to allocate internal buffer.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m *** SIGABRT received at time=1719411971 ***\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m Fatal Python error: Aborted\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m \n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m Stack (most recent call first):\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(launch_and_fit pid=32128)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\worker.py\", line 879 in main_loop\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "ERROR flwr 2024-06-26 19:56:16,390 | ray_client_proxy.py:87 | The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "DEBUG flwr 2024-06-26 19:56:16,399 | server.py:232 | fit_round 2 received 0 results and 10 failures\n",
      "DEBUG flwr 2024-06-26 19:56:16,402 | server.py:168 | evaluate_round 2: strategy sampled 10 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 8f0a191b4d7a62b824fe702c1704ec31365e4d9d01000000 Worker ID: 514145b0510caba696b22469ad40d4bd86fc2c2463dbc666f2341a0d Node ID: 545ae7ca41736b854d81c0bc1e3553d05d65ae68e3db45c76c5d2cfb Worker IP address: 127.0.0.1 Worker port: 65333 Worker PID: 32128 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 10054. An existing connection was forcibly closed by the remote host. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-06-26 19:56:25,428 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=2348, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1836, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1870, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 968, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_adv_train64_8.dll\" or one of its dependencies.\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 457, in subimport\n",
      "    __import__(name)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\__init__.py\", line 128, in <module>\n",
      "    raise err\n",
      "OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_adv_train64_8.dll\" or one of its dependencies.\n",
      "\u001b[36m(launch_and_evaluate pid=2348)\u001b[0m [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_adv_train64_8.dll\" or one of its dependencies.\n",
      "\u001b[36m(launch_and_evaluate pid=2348)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(launch_and_evaluate pid=2348)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 404, in deserialize_objects\n",
      "\u001b[36m(launch_and_evaluate pid=2348)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[36m(launch_and_evaluate pid=2348)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 270, in _deserialize_object\n",
      "\u001b[36m(launch_and_evaluate pid=2348)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[36m(launch_and_evaluate pid=2348)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 225, in _deserialize_msgpack_data\n",
      "\u001b[36m(launch_and_evaluate pid=2348)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "\u001b[36m(launch_and_evaluate pid=2348)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\serialization.py\", line 215, in _deserialize_pickle5_data\n",
      "\u001b[36m(launch_and_evaluate pid=2348)\u001b[0m     obj = pickle.loads(in_band)\n",
      "\u001b[36m(launch_and_evaluate pid=2348)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 457, in subimport\n",
      "\u001b[36m(launch_and_evaluate pid=2348)\u001b[0m     __import__(name)\n",
      "\u001b[36m(launch_and_evaluate pid=2348)\u001b[0m   File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\__init__.py\", line 128, in <module>\n",
      "\u001b[36m(launch_and_evaluate pid=2348)\u001b[0m     raise err\n",
      "\u001b[36m(launch_and_evaluate pid=2348)\u001b[0m OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\lib\\cudnn_adv_train64_8.dll\" or one of its dependencies.\n",
      "ERROR flwr 2024-06-26 19:56:28,327 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=14820, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 26, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "ERROR flwr 2024-06-26 19:56:28,419 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=17740, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 26, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(launch_and_evaluate pid=14820)\u001b[0m [Client 1] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-06-26 19:56:32,123 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=27080, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 26, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "ERROR flwr 2024-06-26 19:56:32,193 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=19960, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 26, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "ERROR flwr 2024-06-26 19:56:32,312 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=22908, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 26, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "ERROR flwr 2024-06-26 19:56:32,492 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=13368, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 26, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "ERROR flwr 2024-06-26 19:56:35,932 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=10324, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 26, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "DEBUG flwr 2024-06-26 19:56:35,940 | server.py:182 | evaluate_round 2 received 0 results and 10 failures\n",
      "DEBUG flwr 2024-06-26 19:56:35,942 | server.py:218 | fit_round 3: strategy sampled 10 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the last time 20.490160942077637\n",
      "This is the last time 21.794506311416626\n",
      "This is the last time 21.68092632293701\n",
      "This is the last time 23.490320444107056\n",
      "This is the last time 22.895992517471313\n",
      "This is the last time 22.2886061668396\n",
      "This is the last time 20.205161809921265\n",
      "This is the last time 23.315999031066895\n",
      "This is the last time 21.924933195114136\n",
      "This is the last time 22.724000215530396\n",
      "\u001b[36m(launch_and_evaluate pid=10324)\u001b[0m [Client 6] evaluate, config: {}\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-06-26 19:56:49,249 | ray_client_proxy.py:87 | \u001b[36mray::launch_and_fit()\u001b[39m (pid=8080, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 148, in launch_and_fit\n",
      "    return maybe_call_fit(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 184, in maybe_call_fit\n",
      "    return client.fit(fit_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 297, in _fit\n",
      "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 14, in fit\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "ERROR flwr 2024-06-26 19:56:49,310 | ray_client_proxy.py:87 | \u001b[36mray::launch_and_fit()\u001b[39m (pid=24688, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 148, in launch_and_fit\n",
      "    return maybe_call_fit(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 184, in maybe_call_fit\n",
      "    return client.fit(fit_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 297, in _fit\n",
      "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 14, in fit\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(launch_and_fit pid=8080)\u001b[0m [Client 6] fit, config: {'lr': 0.0001, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-06-26 19:56:50,256 | ray_client_proxy.py:87 | \u001b[36mray::launch_and_fit()\u001b[39m (pid=1564, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 148, in launch_and_fit\n",
      "    return maybe_call_fit(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 184, in maybe_call_fit\n",
      "    return client.fit(fit_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 297, in _fit\n",
      "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 14, in fit\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "ERROR flwr 2024-06-26 19:56:51,580 | ray_client_proxy.py:87 | \u001b[36mray::launch_and_fit()\u001b[39m (pid=26668, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 148, in launch_and_fit\n",
      "    return maybe_call_fit(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 184, in maybe_call_fit\n",
      "    return client.fit(fit_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 297, in _fit\n",
      "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 14, in fit\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "ERROR flwr 2024-06-26 19:56:52,781 | ray_client_proxy.py:87 | \u001b[36mray::launch_and_fit()\u001b[39m (pid=18736, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 148, in launch_and_fit\n",
      "    return maybe_call_fit(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 184, in maybe_call_fit\n",
      "    return client.fit(fit_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 297, in _fit\n",
      "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 14, in fit\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "ERROR flwr 2024-06-26 19:56:52,872 | ray_client_proxy.py:87 | \u001b[36mray::launch_and_fit()\u001b[39m (pid=21292, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 148, in launch_and_fit\n",
      "    return maybe_call_fit(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 184, in maybe_call_fit\n",
      "    return client.fit(fit_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 297, in _fit\n",
      "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 14, in fit\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "ERROR flwr 2024-06-26 19:56:53,985 | ray_client_proxy.py:87 | \u001b[36mray::launch_and_fit()\u001b[39m (pid=6136, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 148, in launch_and_fit\n",
      "    return maybe_call_fit(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 184, in maybe_call_fit\n",
      "    return client.fit(fit_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 297, in _fit\n",
      "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 14, in fit\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "ERROR flwr 2024-06-26 19:56:55,232 | ray_client_proxy.py:87 | \u001b[36mray::launch_and_fit()\u001b[39m (pid=32588, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 148, in launch_and_fit\n",
      "    return maybe_call_fit(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 184, in maybe_call_fit\n",
      "    return client.fit(fit_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 297, in _fit\n",
      "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 14, in fit\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(launch_and_fit pid=32588)\u001b[0m [Client 0] fit, config: {'lr': 0.0001, 'epochs': 3}\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-06-26 19:56:56,464 | ray_client_proxy.py:87 | \u001b[36mray::launch_and_fit()\u001b[39m (pid=31076, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 148, in launch_and_fit\n",
      "    return maybe_call_fit(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 184, in maybe_call_fit\n",
      "    return client.fit(fit_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 297, in _fit\n",
      "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 14, in fit\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "ERROR flwr 2024-06-26 19:56:56,944 | ray_client_proxy.py:87 | \u001b[36mray::launch_and_fit()\u001b[39m (pid=12732, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 148, in launch_and_fit\n",
      "    return maybe_call_fit(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 184, in maybe_call_fit\n",
      "    return client.fit(fit_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 297, in _fit\n",
      "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 14, in fit\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "DEBUG flwr 2024-06-26 19:56:56,952 | server.py:232 | fit_round 3 received 0 results and 10 failures\n",
      "DEBUG flwr 2024-06-26 19:56:56,955 | server.py:168 | evaluate_round 3: strategy sampled 10 clients (out of 10)\n",
      "ERROR flwr 2024-06-26 19:57:10,224 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=29104, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 26, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(launch_and_evaluate pid=29104)\u001b[0m [Client 7] evaluate, config: {}\n",
      "\u001b[36m(launch_and_fit pid=12732)\u001b[0m [Client 1] fit, config: {'lr': 0.0001, 'epochs': 3}\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-06-26 19:57:11,038 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=28580, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 26, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "ERROR flwr 2024-06-26 19:57:11,101 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=23540, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 26, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "ERROR flwr 2024-06-26 19:57:11,160 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=9500, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 26, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "ERROR flwr 2024-06-26 19:57:13,367 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=12616, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 26, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "ERROR flwr 2024-06-26 19:57:14,885 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=20624, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 26, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "ERROR flwr 2024-06-26 19:57:14,920 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=26588, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 26, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "ERROR flwr 2024-06-26 19:57:14,942 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=9420, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 26, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "ERROR flwr 2024-06-26 19:57:17,310 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=27748, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 26, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(launch_and_evaluate pid=27748)\u001b[0m [Client 1] evaluate, config: {}\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-06-26 19:57:18,747 | ray_client_proxy.py:104 | \u001b[36mray::launch_and_evaluate()\u001b[39m (pid=2924, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1889, in ray._raylet.execute_task\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py\", line 160, in launch_and_evaluate\n",
      "    return maybe_call_evaluate(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\client.py\", line 205, in maybe_call_evaluate\n",
      "    return client.evaluate(evaluate_ins)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\flwr\\client\\app.py\", line 321, in _evaluate\n",
      "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\1380938485.py\", line 26, in evaluate\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_5312\\3083245685.py\", line 28, in set_parameters\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1671, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for Net:\n",
      "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\".\n",
      "DEBUG flwr 2024-06-26 19:57:18,755 | server.py:182 | evaluate_round 3 received 0 results and 10 failures\n",
      "INFO flwr 2024-06-26 19:57:18,767 | server.py:147 | FL finished in 519.3251092\n",
      "INFO flwr 2024-06-26 19:57:18,770 | app.py:218 | app_fit: losses_distributed [(1, 0.05854799574613571)]\n",
      "INFO flwr 2024-06-26 19:57:18,772 | app.py:219 | app_fit: metrics_distributed_fit {}\n",
      "INFO flwr 2024-06-26 19:57:18,773 | app.py:220 | app_fit: metrics_distributed {}\n",
      "INFO flwr 2024-06-26 19:57:18,774 | app.py:221 | app_fit: losses_centralized []\n",
      "INFO flwr 2024-06-26 19:57:18,775 | app.py:222 | app_fit: metrics_centralized {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "History (loss, distributed):\n",
       "\tround 1: 0.05854799574613571"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(launch_and_evaluate pid=2924)\u001b[0m [Client 6] evaluate, config: {}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=10,\n",
    "    config=fl.server.ServerConfig(num_rounds=3),\n",
    "    strategy=FedCustom(),  # <-- pass the new strategy here\n",
    "    client_resources=client_resources,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, valloader):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, epochs=5)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        print(f\"Client {self.cid} loss {loss}\")\n",
    "        print(f\"Client {self.cid} accuracy {accuracy}\")\n",
    "        \n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "def client_fn(cid) -> FlowerClient:\n",
    "    net = Net().to(DEVICE) #Load Model from here\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "    return FlowerClient(cid, net, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-05-29 11:36:33,564 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-05-29 11:36:40,084 | app.py:180 | Flower VCE: Ray initialized with resources: {'GPU': 1.0, 'CPU': 32.0, 'memory': 61953181082.0, 'object_store_memory': 30837077606.0, 'node:127.0.0.1': 1.0}\n",
      "INFO flwr 2024-05-29 11:36:40,087 | server.py:86 | Initializing global parameters\n",
      "INFO flwr 2024-05-29 11:36:40,089 | server.py:273 | Requesting initial parameters from one random client\n",
      "INFO flwr 2024-05-29 11:36:44,026 | server.py:277 | Received initial parameters from one random client\n",
      "INFO flwr 2024-05-29 11:36:44,027 | server.py:88 | Evaluating initial parameters\n",
      "INFO flwr 2024-05-29 11:36:44,029 | server.py:101 | FL starting\n",
      "DEBUG flwr 2024-05-29 11:36:44,031 | server.py:218 | fit_round 1: strategy sampled 10 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pid=27924)\u001b[0m [Client 3] get_parameters\n",
      " pid=27924)\u001b[0m [Client 8] fit, config: {}\n",
      " pid=27000)\u001b[0m [Client 9] fit, config: {}\n",
      " pid=26956)\u001b[0m [Client 5] fit, config: {}\n",
      " pid=19244)\u001b[0m [Client 3] fit, config: {}\n",
      " pid=27924)\u001b[0m Epoch 0: train loss 0.064283, accuracy 0.244000\n",
      " pid=27924)\u001b[0m Epoch 1: train loss 0.056345, accuracy 0.335556\n",
      " pid=27924)\u001b[0m Epoch 2: train loss 0.052596, accuracy 0.383111\n",
      " pid=27000)\u001b[0m Epoch 0: train loss 0.065390, accuracy 0.219333\n",
      " pid=26956)\u001b[0m Epoch 0: train loss 0.065386, accuracy 0.222667\n",
      " pid=27924)\u001b[0m Epoch 3: train loss 0.049705, accuracy 0.431111\n",
      " pid=19244)\u001b[0m Epoch 0: train loss 0.064883, accuracy 0.228667\n",
      " pid=27000)\u001b[0m Epoch 1: train loss 0.057237, accuracy 0.326667\n",
      " pid=26956)\u001b[0m Epoch 1: train loss 0.057874, accuracy 0.322444\n",
      " pid=27924)\u001b[0m Epoch 4: train loss 0.047198, accuracy 0.454667\n",
      " pid=27924)\u001b[0m [Client 1] fit, config: {}\n",
      " pid=19244)\u001b[0m Epoch 1: train loss 0.056371, accuracy 0.348444\n",
      " pid=27000)\u001b[0m Epoch 2: train loss 0.053247, accuracy 0.374667\n",
      " pid=26956)\u001b[0m Epoch 2: train loss 0.054083, accuracy 0.371333\n",
      " pid=27924)\u001b[0m Epoch 0: train loss 0.065366, accuracy 0.229556\n",
      " pid=19244)\u001b[0m Epoch 2: train loss 0.052883, accuracy 0.388889\n",
      " pid=27000)\u001b[0m Epoch 3: train loss 0.050352, accuracy 0.408222\n",
      " pid=26956)\u001b[0m Epoch 3: train loss 0.051255, accuracy 0.404000\n",
      " pid=27924)\u001b[0m Epoch 1: train loss 0.055373, accuracy 0.356444\n",
      " pid=19244)\u001b[0m Epoch 3: train loss 0.050108, accuracy 0.422000\n",
      " pid=27000)\u001b[0m Epoch 4: train loss 0.048146, accuracy 0.439778\n",
      " pid=26956)\u001b[0m Epoch 4: train loss 0.048693, accuracy 0.438000\n",
      " pid=27000)\u001b[0m [Client 0] fit, config: {}\n",
      " pid=26956)\u001b[0m [Client 7] fit, config: {}\n",
      " pid=27924)\u001b[0m Epoch 2: train loss 0.051266, accuracy 0.410000\n",
      " pid=19244)\u001b[0m Epoch 4: train loss 0.047495, accuracy 0.445333\n",
      " pid=19244)\u001b[0m [Client 2] fit, config: {}\n",
      " pid=27000)\u001b[0m Epoch 0: train loss 0.064989, accuracy 0.228000\n",
      " pid=26956)\u001b[0m Epoch 0: train loss 0.064233, accuracy 0.237333\n",
      " pid=27924)\u001b[0m Epoch 3: train loss 0.048039, accuracy 0.435111\n",
      " pid=19244)\u001b[0m Epoch 0: train loss 0.064962, accuracy 0.224667\n",
      " pid=27000)\u001b[0m Epoch 1: train loss 0.056978, accuracy 0.330667\n",
      " pid=26956)\u001b[0m Epoch 1: train loss 0.056086, accuracy 0.346667\n",
      " pid=27924)\u001b[0m Epoch 4: train loss 0.045961, accuracy 0.471111\n",
      " pid=27924)\u001b[0m [Client 4] fit, config: {}\n",
      " pid=19244)\u001b[0m Epoch 1: train loss 0.057172, accuracy 0.331333\n",
      " pid=27000)\u001b[0m Epoch 2: train loss 0.052656, accuracy 0.386444\n",
      " pid=26956)\u001b[0m Epoch 2: train loss 0.052179, accuracy 0.392444\n",
      " pid=27924)\u001b[0m Epoch 0: train loss 0.064859, accuracy 0.233778\n",
      " pid=19244)\u001b[0m Epoch 2: train loss 0.052711, accuracy 0.378889\n",
      " pid=27000)\u001b[0m Epoch 3: train loss 0.049592, accuracy 0.423778\n",
      " pid=26956)\u001b[0m Epoch 3: train loss 0.049415, accuracy 0.424444\n",
      " pid=27924)\u001b[0m Epoch 1: train loss 0.056354, accuracy 0.343333\n",
      " pid=19244)\u001b[0m Epoch 3: train loss 0.049872, accuracy 0.417778\n",
      " pid=27000)\u001b[0m Epoch 4: train loss 0.047783, accuracy 0.446667\n",
      " pid=26956)\u001b[0m Epoch 4: train loss 0.047490, accuracy 0.450444\n",
      " pid=27000)\u001b[0m [Client 6] fit, config: {}\n",
      " pid=27924)\u001b[0m Epoch 2: train loss 0.052208, accuracy 0.391333\n",
      " pid=19244)\u001b[0m Epoch 4: train loss 0.048091, accuracy 0.434222\n",
      " pid=27000)\u001b[0m Epoch 0: train loss 0.065154, accuracy 0.239333\n",
      " pid=27924)\u001b[0m Epoch 3: train loss 0.049021, accuracy 0.424222\n",
      " pid=27000)\u001b[0m Epoch 1: train loss 0.056829, accuracy 0.333556\n",
      " pid=27924)\u001b[0m Epoch 4: train loss 0.046805, accuracy 0.464889\n",
      " pid=27000)\u001b[0m Epoch 2: train loss 0.053426, accuracy 0.377778\n",
      " pid=27000)\u001b[0m Epoch 3: train loss 0.050676, accuracy 0.410444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-05-29 11:37:45,189 | server.py:232 | fit_round 1 received 10 results and 0 failures\n",
      "WARNING flwr 2024-05-29 11:37:45,233 | fedavg.py:243 | No fit_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2024-05-29 11:37:45,235 | server.py:168 | evaluate_round 1: strategy sampled 10 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pid=27000)\u001b[0m Epoch 4: train loss 0.048110, accuracy 0.436444\n",
      " pid=27000)\u001b[0m [Client 4] evaluate, config: {}\n",
      " pid=27000)\u001b[0m Client 4 loss 0.057416656255722044\n",
      " pid=27000)\u001b[0m Client 4 accuracy 0.348\n",
      " pid=27924)\u001b[0m [Client 8] evaluate, config: {}\n",
      " pid=27924)\u001b[0m Client 8 loss 0.05749241328239441\n",
      " pid=27924)\u001b[0m Client 8 accuracy 0.394\n",
      " pid=27924)\u001b[0m [Client 6] evaluate, config: {}\n",
      " pid=27000)\u001b[0m [Client 2] evaluate, config: {}\n",
      " pid=27924)\u001b[0m Client 6 loss 0.05749799847602844\n",
      " pid=27924)\u001b[0m Client 6 accuracy 0.348\n",
      " pid=27000)\u001b[0m Client 2 loss 0.058668370962142946\n",
      " pid=27000)\u001b[0m Client 2 accuracy 0.342\n",
      " pid=27000)\u001b[0m [Client 9] evaluate, config: {}\n",
      " pid=27000)\u001b[0m Client 9 loss 0.05861459255218506\n",
      " pid=27000)\u001b[0m Client 9 accuracy 0.36\n",
      " pid=27000)\u001b[0m [Client 1] evaluate, config: {}\n",
      " pid=27000)\u001b[0m Client 1 loss 0.057283300876617434\n",
      " pid=27000)\u001b[0m Client 1 accuracy 0.376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-05-29 11:38:00,139 | server.py:182 | evaluate_round 1 received 10 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pid=27924)\u001b[0m [Client 5] evaluate, config: {}\n",
      " pid=27000)\u001b[0m [Client 0] evaluate, config: {}\n",
      " pid=26956)\u001b[0m [Client 7] evaluate, config: {}\n",
      " pid=19244)\u001b[0m [Client 3] evaluate, config: {}\n",
      " pid=27924)\u001b[0m Client 5 loss 0.05579921221733093\n",
      " pid=27924)\u001b[0m Client 5 accuracy 0.41\n",
      " pid=27000)\u001b[0m Client 0 loss 0.05796573948860168\n",
      " pid=27000)\u001b[0m Client 0 accuracy 0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING flwr 2024-05-29 11:38:00,142 | fedavg.py:274 | No evaluate_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2024-05-29 11:38:00,144 | server.py:218 | fit_round 2: strategy sampled 10 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pid=26956)\u001b[0m Client 7 loss 0.057370660543441775\n",
      " pid=26956)\u001b[0m Client 7 accuracy 0.362\n",
      " pid=19244)\u001b[0m Client 3 loss 0.057644266366958616\n",
      " pid=19244)\u001b[0m Client 3 accuracy 0.364\n",
      " pid=19244)\u001b[0m [Client 8] fit, config: {}\n",
      " pid=26956)\u001b[0m [Client 9] fit, config: {}\n",
      " pid=19244)\u001b[0m Epoch 0: train loss 0.050530, accuracy 0.418222\n",
      " pid=26956)\u001b[0m Epoch 0: train loss 0.051527, accuracy 0.400000\n",
      " pid=19244)\u001b[0m Epoch 1: train loss 0.046498, accuracy 0.465778\n",
      " pid=27924)\u001b[0m [Client 7] fit, config: {}\n",
      " pid=26956)\u001b[0m Epoch 1: train loss 0.047991, accuracy 0.431556\n",
      " pid=27000)\u001b[0m [Client 2] fit, config: {}\n",
      " pid=19244)\u001b[0m Epoch 2: train loss 0.044626, accuracy 0.484889\n",
      " pid=27924)\u001b[0m Epoch 0: train loss 0.050340, accuracy 0.412667\n",
      " pid=26956)\u001b[0m Epoch 2: train loss 0.045316, accuracy 0.465556\n",
      " pid=27000)\u001b[0m Epoch 0: train loss 0.051500, accuracy 0.403111\n",
      " pid=19244)\u001b[0m Epoch 3: train loss 0.042625, accuracy 0.503778\n",
      " pid=27924)\u001b[0m Epoch 1: train loss 0.047050, accuracy 0.448222\n",
      " pid=26956)\u001b[0m Epoch 3: train loss 0.043097, accuracy 0.494667\n",
      " pid=27000)\u001b[0m Epoch 1: train loss 0.047906, accuracy 0.440000\n",
      " pid=19244)\u001b[0m Epoch 4: train loss 0.040469, accuracy 0.528444\n",
      " pid=19244)\u001b[0m [Client 4] fit, config: {}\n",
      " pid=27924)\u001b[0m Epoch 2: train loss 0.044768, accuracy 0.476000\n",
      " pid=26956)\u001b[0m Epoch 4: train loss 0.040796, accuracy 0.526444\n",
      " pid=26956)\u001b[0m [Client 0] fit, config: {}\n",
      " pid=27000)\u001b[0m Epoch 2: train loss 0.045568, accuracy 0.465778\n",
      " pid=19244)\u001b[0m Epoch 0: train loss 0.050550, accuracy 0.407111\n",
      " pid=27924)\u001b[0m Epoch 3: train loss 0.042242, accuracy 0.513333\n",
      " pid=26956)\u001b[0m Epoch 0: train loss 0.051160, accuracy 0.404667\n",
      " pid=27000)\u001b[0m Epoch 3: train loss 0.043297, accuracy 0.500222\n",
      " pid=19244)\u001b[0m Epoch 1: train loss 0.047062, accuracy 0.453333\n",
      " pid=27924)\u001b[0m Epoch 4: train loss 0.040292, accuracy 0.531778\n",
      " pid=27924)\u001b[0m [Client 1] fit, config: {}\n",
      " pid=26956)\u001b[0m Epoch 1: train loss 0.047471, accuracy 0.444444\n",
      " pid=27000)\u001b[0m Epoch 4: train loss 0.041428, accuracy 0.520889\n",
      " pid=27000)\u001b[0m [Client 6] fit, config: {}\n",
      " pid=19244)\u001b[0m Epoch 2: train loss 0.044409, accuracy 0.482889\n",
      " pid=27924)\u001b[0m Epoch 0: train loss 0.050552, accuracy 0.414222\n",
      " pid=26956)\u001b[0m Epoch 2: train loss 0.045068, accuracy 0.472889\n",
      " pid=27000)\u001b[0m Epoch 0: train loss 0.051794, accuracy 0.393111\n",
      " pid=19244)\u001b[0m Epoch 3: train loss 0.042391, accuracy 0.507556\n",
      " pid=27924)\u001b[0m Epoch 1: train loss 0.047419, accuracy 0.453556\n",
      " pid=26956)\u001b[0m Epoch 3: train loss 0.043068, accuracy 0.504222\n",
      " pid=27000)\u001b[0m Epoch 1: train loss 0.048650, accuracy 0.435778\n",
      " pid=19244)\u001b[0m Epoch 4: train loss 0.040546, accuracy 0.535111\n",
      " pid=19244)\u001b[0m [Client 3] fit, config: {}\n",
      " pid=27924)\u001b[0m Epoch 2: train loss 0.044948, accuracy 0.482000\n",
      " pid=26956)\u001b[0m Epoch 4: train loss 0.040577, accuracy 0.525778\n",
      " pid=26956)\u001b[0m [Client 5] fit, config: {}\n",
      " pid=27000)\u001b[0m Epoch 2: train loss 0.045532, accuracy 0.467778\n",
      " pid=19244)\u001b[0m Epoch 0: train loss 0.051494, accuracy 0.402444\n",
      " pid=27924)\u001b[0m Epoch 3: train loss 0.042631, accuracy 0.497556\n",
      " pid=26956)\u001b[0m Epoch 0: train loss 0.051428, accuracy 0.400000\n",
      " pid=27000)\u001b[0m Epoch 3: train loss 0.043566, accuracy 0.486889\n",
      " pid=19244)\u001b[0m Epoch 1: train loss 0.048357, accuracy 0.435111\n",
      " pid=27924)\u001b[0m Epoch 4: train loss 0.040580, accuracy 0.531333\n",
      " pid=26956)\u001b[0m Epoch 1: train loss 0.048156, accuracy 0.438222\n",
      " pid=27000)\u001b[0m Epoch 4: train loss 0.041662, accuracy 0.513778\n",
      " pid=19244)\u001b[0m Epoch 2: train loss 0.046118, accuracy 0.472444\n",
      " pid=26956)\u001b[0m Epoch 2: train loss 0.046218, accuracy 0.464889\n",
      " pid=19244)\u001b[0m Epoch 3: train loss 0.044190, accuracy 0.493111\n",
      " pid=26956)\u001b[0m Epoch 3: train loss 0.043656, accuracy 0.493556\n",
      " pid=19244)\u001b[0m Epoch 4: train loss 0.042199, accuracy 0.518000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-05-29 11:38:43,789 | server.py:232 | fit_round 2 received 10 results and 0 failures\n",
      "DEBUG flwr 2024-05-29 11:38:43,823 | server.py:168 | evaluate_round 2: strategy sampled 10 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pid=26956)\u001b[0m Epoch 4: train loss 0.041652, accuracy 0.512444\n",
      " pid=26956)\u001b[0m [Client 0] evaluate, config: {}\n",
      " pid=26956)\u001b[0m Client 0 loss 0.046705979347229004\n",
      " pid=26956)\u001b[0m Client 0 accuracy 0.474\n",
      " pid=26956)\u001b[0m [Client 7] evaluate, config: {}\n",
      " pid=26956)\u001b[0m Client 7 loss 0.04484793257713318\n",
      " pid=26956)\u001b[0m Client 7 accuracy 0.48\n",
      " pid=26956)\u001b[0m [Client 1] evaluate, config: {}\n",
      " pid=19244)\u001b[0m [Client 2] evaluate, config: {}\n",
      " pid=27924)\u001b[0m [Client 3] evaluate, config: {}\n",
      " pid=27000)\u001b[0m [Client 5] evaluate, config: {}\n",
      " pid=26956)\u001b[0m Client 1 loss 0.0451300802230835\n",
      " pid=26956)\u001b[0m Client 1 accuracy 0.474\n",
      " pid=27000)\u001b[0m Client 5 loss 0.043583230257034304\n",
      " pid=27000)\u001b[0m Client 5 accuracy 0.508\n",
      " pid=26956)\u001b[0m [Client 6] evaluate, config: {}\n",
      " pid=19244)\u001b[0m Client 2 loss 0.048428908586502076\n",
      " pid=19244)\u001b[0m Client 2 accuracy 0.438\n",
      " pid=27924)\u001b[0m Client 3 loss 0.04525577282905579\n",
      " pid=27924)\u001b[0m Client 3 accuracy 0.48\n",
      " pid=27000)\u001b[0m [Client 8] evaluate, config: {}\n",
      " pid=19244)\u001b[0m [Client 4] evaluate, config: {}\n",
      " pid=27924)\u001b[0m [Client 9] evaluate, config: {}\n",
      " pid=26956)\u001b[0m Client 6 loss 0.04551054120063782\n",
      " pid=26956)\u001b[0m Client 6 accuracy 0.492\n",
      " pid=27000)\u001b[0m Client 8 loss 0.044020456314086916\n",
      " pid=27000)\u001b[0m Client 8 accuracy 0.486\n",
      " pid=19244)\u001b[0m Client 4 loss 0.04514945816993714\n",
      " pid=19244)\u001b[0m Client 4 accuracy 0.466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-05-29 11:38:58,868 | server.py:182 | evaluate_round 2 received 10 results and 0 failures\n",
      "DEBUG flwr 2024-05-29 11:38:58,870 | server.py:218 | fit_round 3: strategy sampled 10 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pid=27924)\u001b[0m Client 9 loss 0.04738649296760559\n",
      " pid=27924)\u001b[0m Client 9 accuracy 0.486\n",
      " pid=27924)\u001b[0m [Client 7] fit, config: {}\n",
      " pid=27924)\u001b[0m Epoch 0: train loss 0.044175, accuracy 0.496000\n",
      " pid=27000)\u001b[0m [Client 9] fit, config: {}\n",
      " pid=19244)\u001b[0m [Client 4] fit, config: {}\n",
      " pid=27924)\u001b[0m Epoch 1: train loss 0.041426, accuracy 0.526000\n",
      " pid=27000)\u001b[0m Epoch 0: train loss 0.045275, accuracy 0.478444\n",
      " pid=19244)\u001b[0m Epoch 0: train loss 0.044497, accuracy 0.490667\n",
      " pid=27924)\u001b[0m Epoch 2: train loss 0.038939, accuracy 0.555778\n",
      " pid=27000)\u001b[0m Epoch 1: train loss 0.042404, accuracy 0.507333\n",
      " pid=26956)\u001b[0m [Client 5] fit, config: {}\n",
      " pid=19244)\u001b[0m Epoch 1: train loss 0.041429, accuracy 0.532222\n",
      " pid=27924)\u001b[0m Epoch 3: train loss 0.036637, accuracy 0.575778\n",
      " pid=27000)\u001b[0m Epoch 2: train loss 0.039452, accuracy 0.541556\n",
      " pid=26956)\u001b[0m Epoch 0: train loss 0.045420, accuracy 0.471778\n",
      " pid=19244)\u001b[0m Epoch 2: train loss 0.039223, accuracy 0.550667\n",
      " pid=27924)\u001b[0m Epoch 4: train loss 0.033954, accuracy 0.610000\n",
      " pid=27924)\u001b[0m [Client 3] fit, config: {}\n",
      " pid=26956)\u001b[0m Epoch 1: train loss 0.042540, accuracy 0.507778\n",
      " pid=27000)\u001b[0m Epoch 3: train loss 0.036782, accuracy 0.568444\n",
      " pid=19244)\u001b[0m Epoch 3: train loss 0.037264, accuracy 0.577333\n",
      " pid=27924)\u001b[0m Epoch 0: train loss 0.045387, accuracy 0.479111\n",
      " pid=26956)\u001b[0m Epoch 2: train loss 0.040411, accuracy 0.526889\n",
      " pid=27000)\u001b[0m Epoch 4: train loss 0.034444, accuracy 0.607556\n",
      " pid=27000)\u001b[0m [Client 0] fit, config: {}\n",
      " pid=19244)\u001b[0m Epoch 4: train loss 0.035378, accuracy 0.597111\n",
      " pid=19244)\u001b[0m [Client 6] fit, config: {}\n",
      " pid=27924)\u001b[0m Epoch 1: train loss 0.043059, accuracy 0.508444\n",
      " pid=26956)\u001b[0m Epoch 3: train loss 0.037816, accuracy 0.561333\n",
      " pid=27000)\u001b[0m Epoch 0: train loss 0.044836, accuracy 0.478667\n",
      " pid=19244)\u001b[0m Epoch 0: train loss 0.045431, accuracy 0.476222\n",
      " pid=27924)\u001b[0m Epoch 2: train loss 0.040902, accuracy 0.529556\n",
      " pid=26956)\u001b[0m Epoch 4: train loss 0.036000, accuracy 0.582889\n",
      " pid=26956)\u001b[0m [Client 1] fit, config: {}\n",
      " pid=27000)\u001b[0m Epoch 1: train loss 0.041439, accuracy 0.519556\n",
      " pid=19244)\u001b[0m Epoch 1: train loss 0.042753, accuracy 0.504889\n",
      " pid=27924)\u001b[0m Epoch 3: train loss 0.038742, accuracy 0.561778\n",
      " pid=26956)\u001b[0m Epoch 0: train loss 0.044765, accuracy 0.477111\n",
      " pid=27000)\u001b[0m Epoch 2: train loss 0.039630, accuracy 0.549111\n",
      " pid=19244)\u001b[0m Epoch 2: train loss 0.039737, accuracy 0.540889\n",
      " pid=27924)\u001b[0m Epoch 4: train loss 0.036108, accuracy 0.592000\n",
      " pid=27924)\u001b[0m [Client 2] fit, config: {}\n",
      " pid=26956)\u001b[0m Epoch 1: train loss 0.041456, accuracy 0.520667\n",
      " pid=27000)\u001b[0m Epoch 3: train loss 0.037016, accuracy 0.572889\n",
      " pid=19244)\u001b[0m Epoch 3: train loss 0.037703, accuracy 0.566889\n",
      " pid=27924)\u001b[0m Epoch 0: train loss 0.044930, accuracy 0.484889\n",
      " pid=26956)\u001b[0m Epoch 2: train loss 0.038729, accuracy 0.550667\n",
      " pid=27000)\u001b[0m Epoch 4: train loss 0.034488, accuracy 0.603556\n",
      " pid=27000)\u001b[0m [Client 8] fit, config: {}\n",
      " pid=19244)\u001b[0m Epoch 4: train loss 0.035678, accuracy 0.589556\n",
      " pid=27924)\u001b[0m Epoch 1: train loss 0.042134, accuracy 0.513556\n",
      " pid=26956)\u001b[0m Epoch 3: train loss 0.036701, accuracy 0.566889\n",
      " pid=27000)\u001b[0m Epoch 0: train loss 0.044234, accuracy 0.490444\n",
      " pid=27924)\u001b[0m Epoch 2: train loss 0.039677, accuracy 0.543778\n",
      " pid=26956)\u001b[0m Epoch 4: train loss 0.034288, accuracy 0.606222\n",
      " pid=27000)\u001b[0m Epoch 1: train loss 0.041474, accuracy 0.519111\n",
      " pid=27924)\u001b[0m Epoch 3: train loss 0.037294, accuracy 0.578444\n",
      " pid=27000)\u001b[0m Epoch 2: train loss 0.038999, accuracy 0.553556\n",
      " pid=27924)\u001b[0m Epoch 4: train loss 0.034748, accuracy 0.608222\n",
      " pid=27000)\u001b[0m Epoch 3: train loss 0.036722, accuracy 0.580444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-05-29 11:39:46,728 | server.py:232 | fit_round 3 received 10 results and 0 failures\n",
      "DEBUG flwr 2024-05-29 11:39:46,764 | server.py:168 | evaluate_round 3: strategy sampled 10 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pid=27000)\u001b[0m Epoch 4: train loss 0.033772, accuracy 0.621778\n",
      " pid=27000)\u001b[0m [Client 7] evaluate, config: {}\n",
      " pid=27000)\u001b[0m Client 7 loss 0.04311247169971466\n",
      " pid=27000)\u001b[0m Client 7 accuracy 0.518\n",
      " pid=27000)\u001b[0m [Client 2] evaluate, config: {}\n",
      " pid=27924)\u001b[0m [Client 5] evaluate, config: {}\n",
      " pid=27000)\u001b[0m Client 2 loss 0.04556352210044861\n",
      " pid=27000)\u001b[0m Client 2 accuracy 0.5\n",
      " pid=26956)\u001b[0m [Client 3] evaluate, config: {}\n",
      " pid=27924)\u001b[0m Client 5 loss 0.0403660671710968\n",
      " pid=27924)\u001b[0m Client 5 accuracy 0.516\n",
      " pid=26956)\u001b[0m Client 3 loss 0.041824417114257814\n",
      " pid=26956)\u001b[0m Client 3 accuracy 0.554\n",
      " pid=26956)\u001b[0m [Client 6] evaluate, config: {}\n",
      " pid=26956)\u001b[0m Client 6 loss 0.04151818835735321\n",
      " pid=26956)\u001b[0m Client 6 accuracy 0.526\n",
      " pid=26956)\u001b[0m [Client 0] evaluate, config: {}\n",
      " pid=26956)\u001b[0m Client 0 loss 0.04397455382347107\n",
      " pid=26956)\u001b[0m Client 0 accuracy 0.51\n",
      " pid=27924)\u001b[0m [Client 9] evaluate, config: {}\n",
      " pid=27000)\u001b[0m [Client 1] evaluate, config: {}\n",
      " pid=26956)\u001b[0m [Client 4] evaluate, config: {}\n",
      " pid=19244)\u001b[0m [Client 8] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-05-29 11:40:01,820 | server.py:182 | evaluate_round 3 received 10 results and 0 failures\n",
      "INFO flwr 2024-05-29 11:40:01,822 | server.py:147 | FL finished in 197.79143010000007\n",
      "INFO flwr 2024-05-29 11:40:01,824 | app.py:218 | app_fit: losses_distributed [(1, 0.05757532110214233), (2, 0.045601885247230536), (3, 0.042524653851985936)]\n",
      "INFO flwr 2024-05-29 11:40:01,825 | app.py:219 | app_fit: metrics_distributed_fit {}\n",
      "INFO flwr 2024-05-29 11:40:01,826 | app.py:220 | app_fit: metrics_distributed {}\n",
      "INFO flwr 2024-05-29 11:40:01,827 | app.py:221 | app_fit: losses_centralized []\n",
      "INFO flwr 2024-05-29 11:40:01,827 | app.py:222 | app_fit: metrics_centralized {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "History (loss, distributed):\n",
       "\tround 1: 0.05757532110214233\n",
       "\tround 2: 0.045601885247230536\n",
       "\tround 3: 0.042524653851985936"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pid=27924)\u001b[0m Client 9 loss 0.04389840006828308\n",
      " pid=27924)\u001b[0m Client 9 accuracy 0.516\n",
      " pid=27000)\u001b[0m Client 1 loss 0.04200864565372467\n",
      " pid=27000)\u001b[0m Client 1 accuracy 0.526\n",
      " pid=26956)\u001b[0m Client 4 loss 0.042090866446495054\n",
      " pid=26956)\u001b[0m Client 4 accuracy 0.498\n",
      " pid=19244)\u001b[0m Client 8 loss 0.040889406085014346\n",
      " pid=19244)\u001b[0m Client 8 accuracy 0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread 2024-05-29 18:33:30,550\tERROR import_thread.py:85 -- ImportThread: <_MultiThreadedRendezvous of RPC that terminated with:\n",
      "\tstatus = StatusCode.UNKNOWN\n",
      "\tdetails = \"Stream removed\"\n",
      "\tdebug_error_string = \"{\"created\":\"@1716987810.545000000\",\"description\":\"Error received from peer ipv4:127.0.0.1:62225\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1075,\"grpc_message\":\"Stream removed\",\"grpc_status\":2}\"\n",
      ">\n",
      "ray_listen_error_messages:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\worker.py\", line 1311, in listen_error_messages_from_gcs\n",
      "    _, error_data = worker.gcs_error_subscriber.poll()\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\gcs_pubsub.py\", line 340, in poll\n",
      "    self._poll_locked(timeout=timeout)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\ray\\_private\\gcs_pubsub.py\", line 270, in _poll_locked\n",
      "    fut.result(timeout=1)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\flwrpytorch\\lib\\site-packages\\grpc\\_channel.py\", line 744, in result\n",
      "    raise self\n",
      "grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n",
      "\tstatus = StatusCode.UNKNOWN\n",
      "\tdetails = \"Stream removed\"\n",
      "\tdebug_error_string = \"{\"created\":\"@1716987810.545000000\",\"description\":\"Error received from peer ipv4:127.0.0.1:62225\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1075,\"grpc_message\":\"Stream removed\",\"grpc_status\":2}\"\n",
      ">\n"
     ]
    }
   ],
   "source": [
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=10,\n",
    "    config=fl.server.ServerConfig(num_rounds=3),\n",
    "    client_resources=client_resources,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
